{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Salmi_Emil_assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtSEYhaVyVwsusULSdhMjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhuggi/nlp/blob/master/Salmi_Emil_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3g-NXuRsIqV"
      },
      "source": [
        "#Excercise 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X14l_0tZz7L2",
        "outputId": "f097c92b-5d08-4304-d447-3075131f2411"
      },
      "source": [
        "!wget -nc https://github.com/TurkuNLP/intro-to-nlp/raw/master/Data/imdb_train.json\r\n",
        "\r\n",
        "import json\r\n",
        "import random\r\n",
        "with open(\"imdb_train.json\") as f:\r\n",
        "    data=json.load(f)\r\n",
        "random.shuffle(data) # Shuffle data\r\n",
        "\r\n",
        "texts=[d[\"text\"] for d in data]\r\n",
        "labels=[d[\"class\"] for d in data]\r\n",
        "\r\n",
        "print(\"Texts length:\", len(texts))\r\n",
        "print(\"Label length:\", len(labels))\r\n",
        "\r\n",
        "for label,text in list(zip(labels,texts))[:20]:\r\n",
        "    print(label,text[:50]+\"...\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘imdb_train.json’ already there; not retrieving.\n",
            "\n",
            "Texts length: 25000\n",
            "Label length: 25000\n",
            "neg Basically this is about a couple who want to adopt...\n",
            "pos I was really impressed with this film. The writing...\n",
            "pos A very strange and compelling movie. It's about a ...\n",
            "neg I had to give this film a 1 because it's that bad,...\n",
            "pos A featherweight plot and dubious characterizations...\n",
            "pos   I have seen this movie many times. At least a Do...\n",
            "neg A mediocre Sci-Fi Channel original picture. A litt...\n",
            "neg My first opinions on this movie were of course bad...\n",
            "neg OK we all love the daisy dukes, but what is up wit...\n",
            "neg The film is almost laughable with Debbie Reynolds ...\n",
            "neg ...was so that I could, in good conscience, tell e...\n",
            "neg Someone will have to explain to me why every film ...\n",
            "neg Its unfortunate that someone decided to spin off o...\n",
            "pos This is the best movie I've seen since White and t...\n",
            "pos It appears that there's no middle ground on this m...\n",
            "neg Well, there you have it, another disillusion on my...\n",
            "neg This movie was made-for-TV, so taking that into ac...\n",
            "pos There are some wonderful things about this movie. ...\n",
            "neg Well! What can one say? Firstly, this adaptation i...\n",
            "pos This short was nominated for an Academy Award and ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5c37L_U0VaD",
        "outputId": "99139e59-9f85-45cb-bae6-9d3d4b3c0f76"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\r\n",
        "vectorizer_tfidf=TfidfVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\r\n",
        "feature_matrix=vectorizer.fit_transform(texts)\r\n",
        "feature_matrix_tfidf=vectorizer_tfidf.fit_transform(texts)\r\n",
        "print(\"Count shape=\",feature_matrix.shape)\r\n",
        "print(\"Count Class:\", feature_matrix.__class__)\r\n",
        "print(\"tfidf shape=\",feature_matrix_tfidf.shape)\r\n",
        "print(\"tfidf Class:\", feature_matrix_tfidf.__class__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count shape= (25000, 74849)\n",
            "Count Class: <class 'scipy.sparse.csr.csr_matrix'>\n",
            "tfidf shape= (25000, 74849)\n",
            "tfidf Class: <class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dePbbvWGICB",
        "outputId": "55176e7a-745f-4e6a-8010-44aaa2216f5e"
      },
      "source": [
        "print(texts[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I was really impressed with this film. The writing was fantastic, and the characters were all rich, and simple. It's very easy to get emotionally attached to all of them. The creators of this movie really hit the nail right on the head when it comes to creating real life characters, and getting the viewer sucked right into their world. Further, the music is terrific. They employed some independents to do the score, and some of the soundtrack, and they do a fantastic job adding to the movie. If you have a chance to catch this movie in a small theater or at a film festival (like I did), I highly recommend that you go see it. Also, on a personal note, Paget Brewster is beautiful in this movie. That's reason enough to go check it out.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrITLMgh2254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06436e9-569c-467d-98ee-4dbd767ba440"
      },
      "source": [
        "print(vectorizer.get_feature_names()[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02', '020410', '029', '03', '04', '041', '05', '050', '06', '06th', '07', '08', '087', '089', '08th', '09', '0f', '0ne', '0r', '0s', '10', '100', '1000', '1000000', '10000000000000', '1000lb', '1000s', '1001', '100b', '100k', '100m', '100min', '100mph', '100s', '100th', '100x', '100yards', '101', '101st', '102', '102nd', '103', '104', '1040', '1040a', '1040s', '105', '1050', '105lbs', '106', '106min', '107', '108', '109', '10am', '10lines', '10mil', '10min', '10minutes', '10p', '10pm', '10s', '10star', '10th', '10x', '10yr', '11', '110', '1100', '11001001', '1100ad', '111', '112', '1138', '114', '1146', '115', '116', '117', '11f', '11m', '11th', '12', '120', '1200', '1200f', '1201', '1202', '123', '12383499143743701', '125', '125m', '127', '128', '12a', '12hr', '12m', '12mm', '12s', '12th', '13', '130', '1300', '1300s', '131', '1318', '132', '134', '135', '135m', '136', '137', '138', '139', '13k', '13s', '13th', '14', '140', '1408', '140hp', '1415', '142', '145', '1454', '146', '147', '1473', '149', '1492', '14a', '14ieme', '14s', '14th', '14yr', '14ème', '15', '150', '1500', '1500s', '150_worst_cases_of_nepotism', '150k', '150m', '151', '152', '153', '1547', '155', '156', '1561', '157', '158', '1594', '15mins', '15minutes', '15s', '15th', '16', '160', '1600', '1600s', '160lbs', '161', '1610', '163', '164', '165', '166', '1660s', '168', '169', '1692', '16ieme', '16k', '16mm', '16s', '16th', '16x9', '16ème', '16éme', '17', '170', '1700', '1700s', '1701', '171', '175', '177', '1775', '1780s', '1790s', '1794', '1798', '17million', '17th', '18', '180', '1800', '1800mph', '1800s', '1801', '1805', '1809', '180d', '1812', '1813', '18137', '1814', '1816', '1820', '1824', '183', '1830', '1832', '1836', '1837', '1838', '1839', '1840', '1840s', '1844', '1846', '1847', '185', '1850', '1850ies', '1850s', '1852', '1853', '1854', '1855', '1859', '1860', '1860s', '1861', '1862', '1863', '1864', '1865', '1870', '1870s', '1871', '1873', '1874', '1875', '1876', '188', '1880', '1880s', '1881', '1886', '1887', '1888', '1889', '188o', '1890', '1890s', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '18a', '18s', '18th', '18year', '19', '190', '1900', '1900s', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1910s', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '192', '1920', '1920ies', '1920s', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1930ies', '1930s', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '193o', '194', '1940', '1940s', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1949er', '195', '1950', '1950s', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1960s', '1961', '1961s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '197', '1970', '1970ies', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '19796', '197o', '1980', '1980ies', '1980s', '1981', '1982', '1982s', '1983', '1983s', '1984', '1984ish', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19k', '19th', '19thc', '1am', '1and', '1d', '1h', '1h30', '1h40', '1h40m', '1h53', '1hour', '1hr', '1million', '1min', '1mln', '1o', '1s', '1st', '1ton', '1tv', '1½', '1ç', '20', '200', '2000', '20000', '20001', '2000ad', '2000s', '2001', '2002', '2003', '2004', '2004s', '2005', '2006', '2007', '2008', '2009', '200ft', '200th', '201', '2010', '2012', '2013', '2015', '2017', '2019', '2020', '2022', '2023', '2030', '2031', '2033', '2035', '2036', '2038', '204', '2040', '2044', '2046', '2047', '2050', '2053', '2054', '206', '2060', '2070', '2080', '209', '2090', '20c', '20ft', '20k', '20m', '20mins', '20minutes', '20mn', '20p', '20perr', '20s', '20th', '20ties', '20widow', '20x', '20year', '20yrs', '21', '210', '2100', '214', '215', '2151', '216', '21699', '21849889', '21849890', '21849907', '21st', '22', '220', '2200', '221', '2210', '22101', '222', '223', '225', '2257', '225mins', '227', '22d', '22h45', '22nd', '23', '230lbs', '230mph', '231', '232', '233', '236', '237', '23d', '23rd', '24', '240', '2400', '241', '242', '248', '2480', '249', '24m30s', '24th', '24years', '25', '250', '2500', '250000', '25million', '25mins', '25s', '25th', '25yo', '25yrs', '26', '260', '2600', '261k', '262', '2642', '269', '26th', '27', '270', '272', '273', '274', '275', '2772', '278', '27th', '27x41', '28', '280', '285', '28th', '29', '29th', '2am', '2d', '2fast', '2furious', '2h', '2h30', '2hour', '2hours', '2hr', '2hrs', '2in', '2inch', '2k', '2more', '2nd', '2oo4', '2oo5', '2pac', '2point4', '2s', '2x4', '30', '300', '3000', '300ad', '300c', '300lbs', '300mln', '3012', '303', '305', '30am', '30ish', '30k', '30lbs', '30min', '30mins', '30pm', '30s', '30something', '30th', '30ties', '31', '3199', '31st', '32', '3200', '320x180', '32lb', '32nd', '33', '330am', '330mins', '332960073452', '336th', '33m', '34', '345', '3462', '34th', '35', '350', '3500', '3516', '356', '357', '35c', '35mins', '35mm', '35pm', '35th', '35yr', '36', '360', '365', '36th', '37', '370', '372', '378', '38', '38k', '38th', '39', '395', '39th', '3am', '3bs', '3d', '3dvd', '3k', '3lbs', '3m', '3mins', '3p', '3p0', '3pm', '3po', '3rd', '3rds', '3th', '3who', '3x5', '3yrs', '40', '400', '4000', '401k', '405', '409', '40am', '40min', '40mins', '40mph', '40s', '40th', '41', '42', '420', '425', '428', '42nd', '43', '430', '44', '440', '442nd', '44c', '44yrs', '45', '450', '4500', '451', '454', '45am', '45min', '45mins', '45s', '46', '465', '469', '47', '475', '477', '47s', '48', '480m', '480p', '48hrs', '49', '498', '49th', '4am', '4cylinder', '4d', '4eva', '4ever', '4f', '4h', '4hrs', '4k', '4kids', '4m', '4o', '4pm', '4th', '4w', '4ward', '4x', '4x4', '50', '500', '5000', '500000', '500ad', '500db', '500lbs', '502', '50c', '50ft', '50ies', '50ish', '50k', '50min', '50mins', '50s', '50th', '50usd', '51', '51b', '51st', '52', '5200', '5250', '529', '52s', '53', '53m', '54', '5400', '540i', '54th', '55', '5539', '555', '55th', '56', '57', '571', '576', '578', '57d', '58', '58th', '59', '598947', '59th', '5hrs', '5ive', '5kph', '5million', '5min', '5mins', '5s', '5seconds', '5th', '5x', '5x5', '5years', '5yo', '5yrs', '60', '600', '6000', '607', '608', '60ies', '60ish', '60mph', '60s', '60th', '60ties', '61', '618', '62', '6200', '62229249', '63', '637', '63rd', '64', '65', '65m', '66', '660', '666', '66er', '67', '6723', '67th', '68', '68th', '69', '69th', '6am', '6b', '6ft', '6hours', '6k', '6million', '6pm', '6th', '6wks', '6yo', '70', '700', '701', '707', '70ies', '70m', '70mm', '70s', '70th', '71', '713', '72', '72nd', '73', '7300', '735', '737', '74', '740', '740il', '747', '747s', '74th', '75', '750', '75054', '75c', '75m', '76', '7600', '77', '78', '788', '78rpm', '79', '79th', '7days', '7even', '7eventy', '7ft', '7ish', '7mm', '7th', '7½th', '80', '800', '8000', '80ies', '80ish', '80min', '80s', '80yr', '81', '817', '819', '82', '820', '8217', '8230', '83', '84', '849', '84f', '84s', '85', '850', '850pm', '86', '86s', '87', '8700', '8763', '878', '87minutes', '88', '88min', '89', '89or', '89s', '8bit', '8ftdf', '8k', '8mm', '8o', '8p', '8pm', '8star', '8th', '8u', '8½', '90', '900', '9000', '90210', '905', '90c', '90ish', '90min', '90mins', '90s', '91', '911', '914', '917', '92', '921', '92fs', '92nd', '93', '937', '94', '9484', '94s', '94th', '95', '950', '95th', '96', '97', '970', '974th', '978', '98', '987', '98minutes', '99', '998', '999', '9999', '99cents', '99p', '99½', '9_', '9am', '9as', '9do', '9ers', '9is', '9lbs', '9mm']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhYLsGoACmak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f500cfe-9b49-47ca-c6d7-487d8a30033b"
      },
      "source": [
        "import sklearn.svm\r\n",
        "\r\n",
        "# Create classifiers with different C values\r\n",
        "classifier=sklearn.svm.LinearSVC(C=0.5, verbose=1)\r\n",
        "classifier2=sklearn.svm.LinearSVC(C=0.01, verbose=1)\r\n",
        "classifier3=sklearn.svm.LinearSVC(C=0.2, verbose=1)\r\n",
        "\r\n",
        "classifier.fit(feature_matrix_tfidf, labels)\r\n",
        "classifier.fit(feature_matrix, labels)\r\n",
        "\r\n",
        "classifier2.fit(feature_matrix_tfidf, labels)\r\n",
        "classifier2.fit(feature_matrix, labels)\r\n",
        "\r\n",
        "classifier3.fit(feature_matrix_tfidf, labels)\r\n",
        "classifier3.fit(feature_matrix, labels)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.2, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wuCkMTQf1Bg",
        "outputId": "926d466d-54e9-49b1-a04e-76dec92b14fc"
      },
      "source": [
        "print(\"C=0.5\")\r\n",
        "print(\"tfidf:\",classifier.score(feature_matrix_tfidf, labels))\r\n",
        "print(\"count:\",classifier.score(feature_matrix, labels))\r\n",
        "print(\"-------------\")\r\n",
        "print(\"C=0.01\")\r\n",
        "print(\"tfidf:\",classifier2.score(feature_matrix_tfidf, labels))\r\n",
        "print(\"count:\",classifier2.score(feature_matrix, labels))\r\n",
        "print(\"-------------\")\r\n",
        "print(\"C=0.2\")\r\n",
        "print(\"tfidf:\",classifier3.score(feature_matrix_tfidf, labels))\r\n",
        "print(\"count:\",classifier3.score(feature_matrix, labels))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C=0.5\n",
            "tfidf: 0.9438\n",
            "count: 1.0\n",
            "-------------\n",
            "C=0.01\n",
            "tfidf: 0.8094\n",
            "count: 0.97012\n",
            "-------------\n",
            "C=0.2\n",
            "tfidf: 0.91612\n",
            "count: 0.99992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP56SZy7gApL",
        "outputId": "e8828960-9fe0-4150-d30f-7f9ec972b948"
      },
      "source": [
        "prediction_count=classifier.predict(feature_matrix)\r\n",
        "prediction_tfidf=classifier.predict(feature_matrix_tfidf)\r\n",
        "\r\n",
        "prediction_count2=classifier2.predict(feature_matrix)\r\n",
        "prediction_tfidf2=classifier2.predict(feature_matrix_tfidf)\r\n",
        "\r\n",
        "prediction_count3=classifier3.predict(feature_matrix)\r\n",
        "prediction_tfidf3=classifier3.predict(feature_matrix_tfidf)\r\n",
        "\r\n",
        "print(\"C=0.5\")\r\n",
        "print(\"Count:\")\r\n",
        "print(prediction_count)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_count))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_count))\r\n",
        "\r\n",
        "print(\"-------------\")\r\n",
        "print(\"TFIDF\")\r\n",
        "print(prediction_tfidf)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_tfidf))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_tfidf))\r\n",
        "print(\"---------------------------------------\")\r\n",
        "print(\"C=0.01\")\r\n",
        "print(\"Count:\")\r\n",
        "print(prediction_count)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_count2))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_count2))\r\n",
        "\r\n",
        "print(\"-------------\")\r\n",
        "print(\"TFIDF\")\r\n",
        "print(prediction_tfidf)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_tfidf2))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_tfidf2))\r\n",
        "print(\"---------------------------------------\")\r\n",
        "print(\"C=0.2\")\r\n",
        "print(\"Count:\")\r\n",
        "print(prediction_count)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_count3))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_count3))\r\n",
        "\r\n",
        "print(\"-------------\")\r\n",
        "print(\"TFIDF\")\r\n",
        "print(prediction_tfidf)\r\n",
        "print(sklearn.metrics.confusion_matrix(labels,prediction_tfidf3))\r\n",
        "print(sklearn.metrics.accuracy_score(labels,prediction_tfidf3))\r\n",
        "print(\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C=0.5\n",
            "Count:\n",
            "['neg' 'pos' 'pos' ... 'neg' 'pos' 'pos']\n",
            "[[12500     0]\n",
            " [    0 12500]]\n",
            "1.0\n",
            "-------------\n",
            "TFIDF\n",
            "['neg' 'pos' 'pos' ... 'pos' 'pos' 'pos']\n",
            "[[11095  1405]\n",
            " [    0 12500]]\n",
            "0.9438\n",
            "---------------------------------------\n",
            "C=0.01\n",
            "Count:\n",
            "['neg' 'pos' 'pos' ... 'neg' 'pos' 'pos']\n",
            "[[12100   400]\n",
            " [  347 12153]]\n",
            "0.97012\n",
            "-------------\n",
            "TFIDF\n",
            "['neg' 'pos' 'pos' ... 'pos' 'pos' 'pos']\n",
            "[[ 7742  4758]\n",
            " [    7 12493]]\n",
            "0.8094\n",
            "---------------------------------------\n",
            "C=0.2\n",
            "Count:\n",
            "['neg' 'pos' 'pos' ... 'neg' 'pos' 'pos']\n",
            "[[12499     1]\n",
            " [    1 12499]]\n",
            "0.99992\n",
            "-------------\n",
            "TFIDF\n",
            "['neg' 'pos' 'pos' ... 'pos' 'pos' 'pos']\n",
            "[[10403  2097]\n",
            " [    0 12500]]\n",
            "0.91612\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMTSpow4qnYK"
      },
      "source": [
        "#Excercise 2\r\n",
        "##Testing with different C values, C=0.2 with the countvectorizer gives almost a perfect result, 0.5 gives a perfect result. tfidf weights are not reaching the same results as countvectorizer, but with C 0.5 it gives a pretty good result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFN_jWnwhAFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc11b2f4-66cd-4bb5-b86c-2d579252688b"
      },
      "source": [
        "vectorizer2=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,2))\r\n",
        "vectorizer3=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,3))\r\n",
        "vectorizer4=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,4))\r\n",
        "vectorizer5=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,5))\r\n",
        "\r\n",
        "classifier=sklearn.svm.LinearSVC(C=0.05, verbose=1)\r\n",
        "classifier2=sklearn.svm.LinearSVC(C=0.01, verbose=1)\r\n",
        "classifier3=sklearn.svm.LinearSVC(C=0.2, verbose=1)\r\n",
        "\r\n",
        "feature_matrix2=vectorizer2.fit_transform(texts)\r\n",
        "feature_matrix3=vectorizer3.fit_transform(texts)\r\n",
        "feature_matrix4=vectorizer4.fit_transform(texts)\r\n",
        "feature_matrix5=vectorizer5.fit_transform(texts)\r\n",
        "\r\n",
        "classifier.fit(feature_matrix2, labels)\r\n",
        "classifier.fit(feature_matrix3, labels)\r\n",
        "classifier.fit(feature_matrix4, labels)\r\n",
        "classifier.fit(feature_matrix5, labels)\r\n",
        "\r\n",
        "classifier2.fit(feature_matrix2, labels)\r\n",
        "classifier2.fit(feature_matrix3, labels)\r\n",
        "classifier2.fit(feature_matrix4, labels)\r\n",
        "classifier2.fit(feature_matrix5, labels)\r\n",
        "\r\n",
        "classifier3.fit(feature_matrix2, labels)\r\n",
        "classifier3.fit(feature_matrix3, labels)\r\n",
        "classifier3.fit(feature_matrix4, labels)\r\n",
        "classifier3.fit(feature_matrix5, labels)\r\n",
        "print(\"\")\r\n",
        "print(\"C=0.05\")\r\n",
        "print(\"range 1-2:\",classifier.score(feature_matrix2, labels))\r\n",
        "print(\"range 1-3:\",classifier.score(feature_matrix3, labels))\r\n",
        "print(\"range 1-4:\",classifier.score(feature_matrix4, labels))\r\n",
        "print(\"range 1-5:\",classifier.score(feature_matrix5, labels))\r\n",
        "print(\"-------------\")\r\n",
        "print(\"C=0.01\")\r\n",
        "print(\"range 1-2:\",classifier2.score(feature_matrix2, labels))\r\n",
        "print(\"range 1-3:\",classifier2.score(feature_matrix3, labels))\r\n",
        "print(\"range 1-4:\",classifier2.score(feature_matrix4, labels))\r\n",
        "print(\"range 1-5:\",classifier2.score(feature_matrix5, labels))\r\n",
        "print(\"-------------\")\r\n",
        "print(\"C=0.2\")\r\n",
        "print(\"range 1-2:\",classifier3.score(feature_matrix2, labels))\r\n",
        "print(\"range 1-3:\",classifier3.score(feature_matrix3, labels))\r\n",
        "print(\"range 1-4:\",classifier3.score(feature_matrix4, labels))\r\n",
        "print(\"range 1-5:\",classifier3.score(feature_matrix5, labels))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]\n",
            "C=0.05\n",
            "range 1-2: 0.52184\n",
            "range 1-3: 0.52372\n",
            "range 1-4: 0.54588\n",
            "range 1-5: 1.0\n",
            "-------------\n",
            "C=0.01\n",
            "range 1-2: 0.52372\n",
            "range 1-3: 0.52292\n",
            "range 1-4: 0.552\n",
            "range 1-5: 0.99928\n",
            "-------------\n",
            "C=0.2\n",
            "range 1-2: 0.52124\n",
            "range 1-3: 0.52432\n",
            "range 1-4: 0.54448\n",
            "range 1-5: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXlcdtb_RYTT"
      },
      "source": [
        "##This looks very suspicious and I don't think this is right, but not really sure what I'm doing wrong\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmlq07wMsByQ"
      },
      "source": [
        "# Excercise 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGN8pwPCEH8n",
        "outputId": "a3bd440e-04bc-407d-caab-102b668ea363"
      },
      "source": [
        "!wget http://dl.turkunlp.org/intro-to-nlp.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 11:46:26--  http://dl.turkunlp.org/intro-to-nlp.tar.gz\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76192539 (73M) [application/octet-stream]\n",
            "Saving to: ‘intro-to-nlp.tar.gz.3’\n",
            "\n",
            "intro-to-nlp.tar.gz 100%[===================>]  72.66M  13.5MB/s    in 10s     \n",
            "\n",
            "2021-02-02 11:46:37 (7.30 MB/s) - ‘intro-to-nlp.tar.gz.3’ saved [76192539/76192539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZNLcIefefxj",
        "outputId": "24b17530-b486-4ae0-83e9-5984c07858c8"
      },
      "source": [
        "!tar zxvf intro-to-nlp.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intro-to-nlp/\n",
            "intro-to-nlp/fi.segmenter.udpipe\n",
            "intro-to-nlp/requirements.txt\n",
            "intro-to-nlp/en.segmenter.udpipe\n",
            "intro-to-nlp/sv.segmenter.udpipe\n",
            "intro-to-nlp/finnish-tweets-sample.jsonl.gz\n",
            "intro-to-nlp/imdb_train.json\n",
            "intro-to-nlp/english-tweets-sample.jsonl.gz\n",
            "intro-to-nlp/language-identification/\n",
            "intro-to-nlp/language-identification/pt_test.txt\n",
            "intro-to-nlp/language-identification/et_train.txt\n",
            "intro-to-nlp/language-identification/fi_train.txt\n",
            "intro-to-nlp/language-identification/fi_devel.txt\n",
            "intro-to-nlp/language-identification/et_devel.txt\n",
            "intro-to-nlp/language-identification/es_test.txt\n",
            "intro-to-nlp/language-identification/es_train.txt\n",
            "intro-to-nlp/language-identification/pt_train.txt\n",
            "intro-to-nlp/language-identification/fi_test.txt\n",
            "intro-to-nlp/language-identification/es_devel.txt\n",
            "intro-to-nlp/language-identification/pt_devel.txt\n",
            "intro-to-nlp/language-identification/en_devel.txt\n",
            "intro-to-nlp/language-identification/en_test.txt\n",
            "intro-to-nlp/language-identification/et_test.txt\n",
            "intro-to-nlp/language-identification/en_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoaCWrGWe2o0",
        "outputId": "6c0bb193-c8be-4407-c549-387571e09ac7"
      },
      "source": [
        "import glob\r\n",
        "import os\r\n",
        "\r\n",
        "texts_lan_train = []\r\n",
        "texts_lan_devel = []\r\n",
        "texts_lan_test = []\r\n",
        "\r\n",
        "labels_lan_test = []\r\n",
        "labels_lan_devel = []\r\n",
        "labels_lan_train = []\r\n",
        "\r\n",
        "def read_data(lang, sec):\r\n",
        "  spl_sec = sec.split('.')[0]\r\n",
        "  label = f\"{lang}_{spl_sec}\"\r\n",
        "\r\n",
        "  fname=f\"intro-to-nlp/language-identification/{lang}_{sec}\"\r\n",
        "  with open(fname) as f:\r\n",
        "    if \"train\" in label:\r\n",
        "      labels_lan_train.append(label)\r\n",
        "      texts_lan_train.append(f.read())\r\n",
        "    if \"devel\" in label:\r\n",
        "      labels_lan_devel.append(label)\r\n",
        "      texts_lan_devel.append(f.read())\r\n",
        "\r\n",
        "t_train = []\r\n",
        "l_train = []\r\n",
        "t_dev = []\r\n",
        "l_dev = []\r\n",
        "\r\n",
        "\r\n",
        "all_files=glob.glob(\"intro-to-nlp/language-identification/*.txt\")\r\n",
        "for fname in all_files:\r\n",
        "  base=os.path.basename(fname)\r\n",
        "  cls=base.split(\"_\")\r\n",
        "  read_data(cls[0], cls[1])\r\n",
        "\r\n",
        "for i, d in enumerate(texts_lan_train):\r\n",
        "  l = labels_lan_train[i]\r\n",
        "  rows = d.split(\".\")\r\n",
        "  for r in rows:\r\n",
        "    t_train.append(r)\r\n",
        "    l_train.append(l)\r\n",
        "for i, d in enumerate(texts_lan_devel):\r\n",
        "  l = labels_lan_devel[i]\r\n",
        "  rows = d.split(\".\")\r\n",
        "  for r in rows:\r\n",
        "    t_dev.append(r)\r\n",
        "    l_dev.append(l)\r\n",
        "\r\n",
        "for label,text in list(zip(l_train,t_train))[:20]:\r\n",
        "    print(label,text[:50]+\"...\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pt_train Ele sabe que não lhe dei o meu voto...\n",
            "pt_train \n",
            "O secretário de o Tesouro, Lloyd Bentsen, disse q...\n",
            "pt_train \n",
            "Com a mesma hombridade que exibe mais uma vez...\n",
            "pt_train \n",
            "Foi sol de pouca dura...\n",
            "pt_train \n",
            "Quem disse que as transferências eram só em o def...\n",
            "pt_train \n",
            "Organizados em associação, os franqueados ganham ...\n",
            "pt_train \n",
            "Essa decisão deverá dar mais de US$ 12 bilhões a ...\n",
            "pt_train \n",
            "Pai E Filho 1\n",
            "As intervenções são feitas com base...\n",
            "pt_train \n",
            "Principalmente em o Carnaval!\n",
            "O cinema de Renoir...\n",
            "pt_train \n",
            "As centenas de milhares de visitantes esperados a...\n",
            "pt_train \n",
            "Segundo Migues, algumas fábricas de a Autolatina ...\n",
            "pt_train \n",
            "Em Agosto, por exemplo, Teerão anunciou uma troca...\n",
            "pt_train \n",
            "Itamar emprega seus amigos em o futuro governo de...\n",
            "pt_train B...\n",
            "pt_train  Duarte e a seus dirigentes por a prática de ativi...\n",
            "pt_train \n",
            "« Obrigado por a vossa visita a Timor-Leste»...\n",
            "pt_train \n",
            "Em a semana passada, a Secretaria Nacional de Dir...\n",
            "pt_train \n",
            "Sendo a fotografia um lugar de registo de os corp...\n",
            "pt_train \n",
            "Jorge Petiz, em Porsche Carrera RSR, concorrente ...\n",
            "pt_train \n",
            "Tem que se demonstrar através de contas e de raci...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMGIaOnOF3rM",
        "outputId": "4d83123b-64a0-41db-d8b7-8bf83f4930bb"
      },
      "source": [
        "for label,text in list(zip(labels_lan_devel,texts_lan_devel)):\r\n",
        "    print(label,text[:50]+\"...\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pt_devel A notícia de a sua morte só ontem foi conhecida, a...\n",
            "es_devel En 1975, volvió a la Argentina para jugar en Unión...\n",
            "fi_devel Ennen hakemuksen esittämistä se haluaa muodolliset...\n",
            "en_devel Let me know if you have any questions.\n",
            "Obudu cattl...\n",
            "et_devel Ta töötas selle kallal ihuüksi kuus aastat, elatud...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP6qkeD1r3j6"
      },
      "source": [
        "# Excercise 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U7jhnRkibPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58c21a5-960b-44c7-f459-bfc326922f66"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "vectorizer_lan=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\r\n",
        "vectorizer_lan1=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,3))\r\n",
        "vectorizer_lan2=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,5))\r\n",
        "\r\n",
        "feature_matrix_lan=vectorizer_lan.fit_transform(t_train)\r\n",
        "feature_matrix_lan_dev=vectorizer_lan.transform(t_dev)\r\n",
        "\r\n",
        "feature_matrix_lan1=vectorizer_lan1.fit_transform(t_train)\r\n",
        "feature_matrix_lan_dev1=vectorizer_lan1.fit_transform(t_dev)\r\n",
        "\r\n",
        "feature_matrix_lan2=vectorizer_lan1.fit_transform(t_train)\r\n",
        "feature_matrix_lan_dev2=vectorizer_lan1.fit_transform(t_dev)\r\n",
        "\r\n",
        "\r\n",
        "print(\"Train=\",feature_matrix_lan.shape)\r\n",
        "print(\"Count Class:\", feature_matrix_lan.__class__)\r\n",
        "print(\"Dev=\",feature_matrix_lan_dev.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train= (4790, 28620)\n",
            "Count Class: <class 'scipy.sparse.csr.csr_matrix'>\n",
            "Dev= (4716, 28620)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx4sP6RDqSFn"
      },
      "source": [
        "# Added different ngram ranges."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZB4gyhUvHB8",
        "outputId": "bf05d522-2564-4c19-8449-863a4b346559"
      },
      "source": [
        "#Check data\r\n",
        "print(vectorizer_lan.get_feature_names()[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '000', '0025', '003', '01', '013', '0140', '01mar01', '02', '03', '04', '040', '0417', '048', '049', '05', '057', '06', '07', '08', '084s', '09', '0c', '0m', '0nside', '10', '100', '1000', '1007', '100ml', '102', '10458', '107korruselise', '1093', '10am', '11', '110', '112', '113', '1139', '1147', '1167', '118', '119', '11iv3', '12', '120', '1200', '12282', '125', '126', '1260', '1264', '127', '1279', '1289', '129', '13', '130', '13061', '131', '1311', '1365', '1375565', '138', '139', '14', '1400', '143', '144', '146', '147', '14h30', '15', '150', '1500', '1514', '1520', '1530', '1550', '1554', '1576', '1580', '1582', '15h00', '15º', '16', '1600', '1601', '1604', '1605', '1617', '162', '1629', '1643', '1649', '1661', '1690', '1695', '17', '170', '1727', '173', '1734', '1736', '1737', '175', '1750s', '1759', '1775', '1783', '1798', '1799', '17h00', '18', '180', '1800', '1800ndad', '1810', '1815', '1817', '1818', '182', '1821', '1822', '1823', '1829', '1833', '1835', '1836', '1840', '1860', '1862', '1863', '1871', '1875', '1876', '1882', '1884', '1887', '1889', '1890', '1891', '1892', '1893', '1894', '1895', '1896', '1897', '18978', '19', '1900', '1901', '1903', '1904', '1907', '1908', '1909', '1910', '1911', '1913', '1914', '1917', '1918', '1919', '1920', '1921', '1923', '1925', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1935', '1936', '1937', '1938', '1939', '1940', '1941', '1943', '1944', '1945', '1946', '1948', '195', '1950', '1950ndatele', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1960', '1960ndatest', '1962', '1963', '1964', '1966', '1967', '1968', '1969', '197', '1970', '1970s', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '199', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19th', '1m44', '1m45', '1q', '1st', '1v', '1ª', '1º', '20', '200', '2000', '20000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '200mm', '2010', '2011', '2012', '2013', '202', '208', '20800', '2093', '20h', '20h00', '20h30', '21', '210', '215', '217', '21h', '21h00', '21ª', '22', '22301', '225', '22h', '23', '231', '235', '2377', '23h', '24', '240', '247', '248', '25', '250', '2500', '253', '2543', '25s', '26', '269', '26th', '27', '270', '272', '28', '281', '287', '28th', '29', '290', '2c', '2p', '30', '300', '301', '303', '3044', '3067', '307', '308', '309', '30aastased', '30pm', '31', '310', '3143c', '32', '320', '321', '323', '325', '327', '328', '33', '34', '341', '345', '35', '350', '350sqft', '354', '357', '359', '36', '360', '362', '363', '3679', '368', '37', '3703', '3744', '38', '380', '386', '3969', '3rd', '3º', '40', '400', '4000', '401', '41', '4101', '416', '416mg', '417', '418', '4193', '41aastane', '42', '422', '43', '44', '443', '444', '448', '45', '450', '46', '4602', '47', '4732', '474', '48', '482', '49', '495', '4h30', '4m', '4th', '4º', '50', '500', '5000', '50000', '5061', '51', '5122', '52', '5218', '524', '53', '54', '547', '55', '56', '566', '57', '572', '575', '58', '586', '589', '5am', '5º', '60', '600', '6000', '600000', '61', '619', '6197', '62', '626', '627', '63', '633', '634', '637', '638', '64', '642', '644', '646', '65', '6500', '6585', '66', '6614', '663', '668', '68', '69', '690', '6º', '70', '700', '7000', '7037686710', '70ndate', '71', '713', '72', '726', '729', '73', '7381', '76', '7614', '776', '78', '787', '79', '799', '7am', '80', '800', '802', '82', '827', '82c55a', '83', '830', '834', '84', '8420', '85', '850', '853', '86', '865', '87', '88', '880', '887s', '899', '8am', '8ball', '8h30', '8th', '90', '90a', '90ndail', '91', '912', '916', '92', '920', '9221', '923', '93', '936', '94', '9499', '95', '950', '965', '968', '97', '98', '9890', '99', '990', '9945', '996', '9h', '9º', '_______', '___________', 'aaa', 'aafia', 'aaiún', 'aallosta', 'aamiaiset', 'aamulla', 'aamuviideltä', 'aamuyöllä', 'aang', 'aapo', 'aarne', 'aaron', 'aasa', 'aasian', 'aasiast', 'aasta', 'aastaaruannet', 'aastad', 'aastaga', 'aastaks', 'aastakümne', 'aastakümnete', 'aastal', 'aastane', 'aastani', 'aastas', 'aastase', 'aastasse', 'aastast', 'aastat', 'aastate', 'aastatel', 'aastatest', 'aatetta', 'aatteelliseksi', 'aavista', 'aavistaa', 'aavo', 'abades', 'abajo', 'abalada', 'abanadonada', 'abanderada', 'abandona', 'abandonada', 'abandonando', 'abandonar', 'abandonaron', 'abandonaría', 'abandono', 'abandoné', 'abandonó', 'abarca', 'abbotsford', 'abbudi', 'abcesso', 'abdel', 'abdomen', 'abdul', 'abdurzakovi', 'abebassa', 'abelardo', 'abercromby', 'abertas', 'aberto', 'abertura', 'abhinav', 'abi', 'abiellumise', 'abielus', 'abierta', 'abierto', 'abikaasa', 'abiks', 'abil', 'abiline', 'abilities', 'ability', 'abiraha', 'abisaamise', 'abja', 'able', 'aboard', 'abogado', 'aboim', 'abolicionistas', 'abordagens', 'aborto', 'about', 'above', 'abra', 'abram', 'abraçado', 'abraçar', 'abreu', 'abreviado', 'abril', 'abrir', 'abriu', 'abrió', 'abroad', 'abruma', 'abruzos', 'absence', 'absolutamente', 'absolutely', 'absoluto', 'absolvido', 'absorver', 'abstain', 'abstención', 'abstenido', 'abstractas', 'abstraction', 'abstratos', 'abuela', 'abuelos', 'abundan', 'abundancia', 'aburrido', 'abuse', 'abusing', 'abusivo', 'abuso', 'ac', 'acaba', 'acabado', 'acabar', 'acabo', 'acabou', 'acabó', 'academia', 'academias', 'academy', 'académica', 'acadêmica', 'acadêmico', 'acantilado', 'acantilados', 'acantonados', 'acareação', 'acatada', 'acceder', 'accelerated', 'accent', 'accenturen', 'acceptable', 'accepted', 'acceso', 'access', 'accident', 'accidentally', 'accidentalmente', 'accidente', 'accionado', 'acción', 'accomplish', 'accomplishment', 'according', 'account', 'accounting', 'accounts', 'accumulated', 'accurate', 'accused', 'ace', 'aceita', 'aceitaram', 'acelerada', 'aceleramos', 'acelerar', 'acentuam', 'acentuar', 'acepta', 'aceptación', 'aceptar', 'acer', 'acerca', 'acercamiento', 'acercár', 'acercársele', 'acerta', 'acertada', 'acertado', 'acertavam', 'acertó', 'acetosellan', 'acha', 'achieve', 'achieved', 'acho', 'acidente', 'acidentes', 'acidez', 'acima', 'acirrada', 'acis', 'aclarar', 'acogedor', 'acogerán', 'acogido', 'acompanhada', 'acompanhado', 'acompanhar', 'acompanharam', 'acompanhavam', 'acompanhe', 'acompanhem', 'acompañaban', 'acompañado', 'acompañar', 'acompañarle', 'acompañe', 'aconcagua', 'aconsejan', 'aconselha', 'aconselharem', 'acontece', 'aconteceu', 'acontecimientos', 'acordaram', 'acordes', 'acordo', 'acordos', 'acorralar', 'acquaintance', 'acquisition', 'acredita', 'acreditam', 'acreditándo', 'acreditándola', 'acrescenta', 'acrescentado', 'acrescento', 'acrescentou', 'acrescida', 'acrescidamente', 'acrescido', 'across', 'acs', 'act', 'acting', 'action', 'actitud', 'activa', 'active', 'actividad', 'actividade', 'actividades', 'activistas', 'activities', 'activity', 'activo', 'acto', 'actor', 'actores', 'actos', 'actriz', 'acts', 'actua', 'actuaciones', 'actuais', 'actual', 'actuales', 'actualidad', 'actualidade', 'actually', 'actualmente', 'actuando', 'actuar', 'actuação', 'acuario', 'acudieron', 'acuerdo', 'acuerdos', 'aculturización', 'acumulación', 'acumuladas', 'acumulan', 'acumulara', 'acupuncture', 'acupuncturist', 'acusa', 'acusaciones', 'acusadas', 'acusado', 'acusados', 'acusam', 'acusando', 'acusações', 'acusou', 'acuáticas', 'acção', 'acólitos', 'adam', 'adams', 'adansonia', 'adaptado', 'adaptados', 'adaptar', 'adaptarmos', 'adarnase', 'added', 'addikti', 'adding', 'addis', 'additional', 'address', 'addressed', 'adecuada', 'adecuadas', 'adelino', 'además', 'adenauerin', 'adentro', 'adequadamente', 'adequadas', 'aderirem', 'adesão', 'adhesión', 'adiamento', 'adianta', 'adiantada', 'adiantadas', 'adiantado', 'adiar', 'adicional', 'adición', 'adjunto', 'adjust', 'adjustment', 'adm', 'admin', 'administer', 'administración', 'administrado', 'administration', 'administrativa', 'administrativas', 'administrative', 'administrativo', 'administrativos', 'administração', 'administró', 'admiraliteedi', 'admiração', 'admitia', 'admitiu', 'admittedly', 'adnan', 'adobado', 'adobe', 'adolescencia', 'adolescente', 'adolescentes', 'adolf', 'adolfo', 'adoptado', 'adoptan', 'adopção', 'adorable', 'adoração', 'adormecer', 'adotada', 'adquiere', 'adquiriente', 'adquirir', 'adquirió', 'adquisición', 'adressaadile', 'ads', 'aduaneira', 'adult', 'adulta', 'adults', 'advanced', 'advancements', 'adventure', 'adventurous', 'adversária', 'adversário', 'adversários', 'advertir', 'advertiu', 'advertência', 'advice', 'advinar', 'advirtió', 'advise', 'advisors', 'advogada', 'advogado', 'aeda', 'aeg', 'aega', 'aeganõudvaim', 'aegsel', 'aegu', 'aero', 'aeromolvia', 'aeronaves', 'aeroporto', 'aeropuerto', 'aerospace', 'afastada', 'afastadas', 'afastado', 'afastamento', 'afastando', 'afastou', 'afección', 'afectada', 'afectados', 'afecto', 'afectó', 'afeitar', 'aferição', 'affair', 'affluent', 'affords', 'afghan', 'afghanistan', 'aficionados', 'afigure', 'afinal', 'afincada', 'afirma', 'afirmado', 'afirmam', 'afirman', 'afirmar', 'afirmação', 'afirmações', 'afirmou', 'afirmó', 'afluencia', 'afluência', 'afonso', 'afora', 'afraid', 'africana', 'africano', 'afrikan', 'afroamericanos', 'after', 'afterall', 'afueras', 'afán', 'ag', 'aga', 'agaiatado', 'again', 'against', 'age', 'aged', 'agencia', 'agency', 'agenda', 'agent', 'agentes', 'ages', 'aggressiiviselta', 'agnes', 'agnesille', 'agni', 'ago', 'agora', 'agosto', 'agradable', 'agradado', 'agradecer', 'agradeci', 'agrado', 'agrarias', 'agrediu', 'agree', 'agreed', 'agreement']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfFB-YFlBUWY",
        "outputId": "c282bd2f-a5b1-4bb8-dd94-45b78ed9887b"
      },
      "source": [
        "import sklearn.svm\r\n",
        "\r\n",
        "# Create classifiers with different C values\r\n",
        "classifier_lan=sklearn.svm.LinearSVC(C=0.05, verbose=1)\r\n",
        "classifier_lan_dev=sklearn.svm.LinearSVC(C=0.5, verbose=1)\r\n",
        "\r\n",
        "classifier_lan1=sklearn.svm.LinearSVC(C=0.05, verbose=1)\r\n",
        "classifier_lan_dev1=sklearn.svm.LinearSVC(C=0.5, verbose=1)\r\n",
        "\r\n",
        "classifier_lan2=sklearn.svm.LinearSVC(C=0.05, verbose=1)\r\n",
        "classifier_lan_dev2=sklearn.svm.LinearSVC(C=0.5, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "classifier_lan.fit(feature_matrix_lan, l_train)\r\n",
        "classifier_lan_dev.fit(feature_matrix_lan_dev, l_dev)\r\n",
        "\r\n",
        "classifier_lan1.fit(feature_matrix_lan1, l_train)\r\n",
        "classifier_lan_dev1.fit(feature_matrix_lan_dev1, l_dev)\r\n",
        "\r\n",
        "classifier_lan2.fit(feature_matrix_lan2, l_train)\r\n",
        "classifier_lan_dev2.fit(feature_matrix_lan_dev2, l_dev)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjmD1iZIqV_j"
      },
      "source": [
        "##Running with different ngram ranges but same C-values to see the difference that ngram does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9TZMsx8BUg2",
        "outputId": "2b897cc0-68c8-4c80-9a91-2d6e79c6ee04"
      },
      "source": [
        "print(\"TRAIN, ngram 1,1: \",classifier_lan.score(feature_matrix_lan, l_train))\r\n",
        "print(\"DEV, ngram 1,1:\",classifier_lan_dev.score(feature_matrix_lan_dev, l_dev))\r\n",
        "print()\r\n",
        "print(\"TRAIN, ngram 1,3:\",classifier_lan1.score(feature_matrix_lan1, l_train))\r\n",
        "print(\"DEV, ngram 1,3:\",classifier_lan_dev1.score(feature_matrix_lan_dev1, l_dev))\r\n",
        "print()\r\n",
        "print(\"TRAIN, ngram 1,5:\",classifier_lan2.score(feature_matrix_lan2, l_train))\r\n",
        "print(\"DEV, ngram 1,5:\",classifier_lan_dev2.score(feature_matrix_lan_dev2, l_dev))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN, ngram 1,1:  0.9588726513569937\n",
            "DEV, ngram 1,1: 0.950381679389313\n",
            "\n",
            "TRAIN, ngram 1,3: 0.9574112734864301\n",
            "DEV, ngram 1,3: 0.9681933842239185\n",
            "\n",
            "TRAIN, ngram 1,5: 0.9574112734864301\n",
            "DEV, ngram 1,5: 0.9681933842239185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfhJtYQrQ3W8"
      },
      "source": [
        "##Seems like increasing the ngram over 3 words doesn't make a difference and will have the same result as 5 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKQqLt2YoU6Q",
        "outputId": "4bf91373-625c-43f7-9a09-98988938b11a"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "predictions_dev=classifier_lan_dev.predict(feature_matrix_lan_dev)\r\n",
        "print(predictions_dev)\r\n",
        "print(sklearn.metrics.confusion_matrix(l_dev,predictions_dev))\r\n",
        "print(sklearn.metrics.accuracy_score(l_dev,predictions_dev))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pt_devel' 'pt_devel' 'pt_devel' ... 'et_devel' 'et_devel' 'fi_devel']\n",
            "[[ 783    1    1   95    1]\n",
            " [   0 1035    0   39    0]\n",
            " [   0    0  881   52    0]\n",
            " [   0    0    2  927    0]\n",
            " [   0    1    0   42  856]]\n",
            "0.950381679389313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBCGjImepbX3"
      },
      "source": [
        "##Pretty good results. If I understood this correctly it seems like English is where the biggest issues are, other languages have only 1-2 words wrong, but other languages get mistaken as english 40-100 times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmXEG-aHrteT"
      },
      "source": [
        "# Excercise 5 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Fs0jTU0mNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e9e54f-063b-46de-ce3d-fe9f6cecdd5d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train_texts, dev_texts, train_labels, dev_labels=train_test_split(t_train,l_train,test_size=0.2)\r\n",
        "vectorizer=CountVectorizer(max_features=100000,binary=True,ngram_range=(1,1))\r\n",
        "feature_matrix_train=vectorizer.fit_transform(train_texts)\r\n",
        "feature_matrix_dev=vectorizer.transform(dev_texts)\r\n",
        "\r\n",
        "print(feature_matrix_train.shape)\r\n",
        "print(feature_matrix_dev.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3832, 24207)\n",
            "(958, 24207)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwxO-LZ6m8DO",
        "outputId": "288c2469-6eca-4c93-b95c-40ffc853d315"
      },
      "source": [
        "import sklearn.svm\r\n",
        "classifier=sklearn.svm.LinearSVC(C=0.2,verbose=1)\r\n",
        "classifier.fit(feature_matrix_train, train_labels)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.2, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2LaSbUhnKkr",
        "outputId": "c8e74395-228c-4959-d45c-f2721e7f4b5a"
      },
      "source": [
        "print(\"TRAIN\",classifier.score(feature_matrix_train, train_labels))\r\n",
        "print(\"DEV\",classifier.score(feature_matrix_dev, dev_labels))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN 0.9692066805845512\n",
            "DEV 0.9164926931106472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWGkv3-ynXzd",
        "outputId": "95525dac-5f92-41b2-aba0-46f77eb5a4b1"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "predictions_dev=classifier.predict(feature_matrix_dev)\r\n",
        "print(predictions_dev)\r\n",
        "print(sklearn.metrics.confusion_matrix(dev_labels,predictions_dev))\r\n",
        "print(sklearn.metrics.accuracy_score(dev_labels,predictions_dev))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pt_train' 'es_train' 'fi_train' 'et_train' 'es_train' 'es_train'\n",
            " 'pt_train' 'en_train' 'pt_train' 'en_train' 'fi_train' 'en_train'\n",
            " 'fi_train' 'pt_train' 'et_train' 'en_train' 'en_train' 'fi_train'\n",
            " 'pt_train' 'pt_train' 'pt_train' 'en_train' 'et_train' 'pt_train'\n",
            " 'en_train' 'pt_train' 'et_train' 'et_train' 'en_train' 'es_train'\n",
            " 'en_train' 'et_train' 'en_train' 'fi_train' 'es_train' 'es_train'\n",
            " 'fi_train' 'fi_train' 'pt_train' 'et_train' 'en_train' 'fi_train'\n",
            " 'en_train' 'pt_train' 'es_train' 'pt_train' 'es_train' 'en_train'\n",
            " 'fi_train' 'pt_train' 'es_train' 'fi_train' 'pt_train' 'fi_train'\n",
            " 'et_train' 'et_train' 'es_train' 'pt_train' 'pt_train' 'et_train'\n",
            " 'pt_train' 'et_train' 'es_train' 'fi_train' 'fi_train' 'pt_train'\n",
            " 'et_train' 'fi_train' 'en_train' 'es_train' 'en_train' 'et_train'\n",
            " 'es_train' 'es_train' 'en_train' 'es_train' 'en_train' 'et_train'\n",
            " 'pt_train' 'et_train' 'et_train' 'fi_train' 'pt_train' 'pt_train'\n",
            " 'pt_train' 'en_train' 'fi_train' 'en_train' 'en_train' 'pt_train'\n",
            " 'fi_train' 'pt_train' 'pt_train' 'fi_train' 'et_train' 'en_train'\n",
            " 'fi_train' 'es_train' 'pt_train' 'et_train' 'en_train' 'fi_train'\n",
            " 'en_train' 'es_train' 'en_train' 'fi_train' 'es_train' 'en_train'\n",
            " 'et_train' 'fi_train' 'es_train' 'en_train' 'fi_train' 'pt_train'\n",
            " 'es_train' 'es_train' 'es_train' 'pt_train' 'es_train' 'et_train'\n",
            " 'es_train' 'pt_train' 'et_train' 'et_train' 'et_train' 'en_train'\n",
            " 'fi_train' 'et_train' 'fi_train' 'fi_train' 'es_train' 'pt_train'\n",
            " 'en_train' 'en_train' 'fi_train' 'es_train' 'pt_train' 'en_train'\n",
            " 'pt_train' 'et_train' 'pt_train' 'fi_train' 'fi_train' 'es_train'\n",
            " 'pt_train' 'pt_train' 'es_train' 'fi_train' 'fi_train' 'pt_train'\n",
            " 'et_train' 'es_train' 'et_train' 'pt_train' 'et_train' 'en_train'\n",
            " 'en_train' 'et_train' 'en_train' 'en_train' 'en_train' 'pt_train'\n",
            " 'fi_train' 'en_train' 'es_train' 'en_train' 'es_train' 'et_train'\n",
            " 'es_train' 'fi_train' 'et_train' 'en_train' 'et_train' 'es_train'\n",
            " 'fi_train' 'pt_train' 'en_train' 'es_train' 'pt_train' 'pt_train'\n",
            " 'es_train' 'et_train' 'et_train' 'es_train' 'es_train' 'es_train'\n",
            " 'es_train' 'es_train' 'fi_train' 'et_train' 'et_train' 'pt_train'\n",
            " 'et_train' 'et_train' 'fi_train' 'et_train' 'fi_train' 'en_train'\n",
            " 'es_train' 'et_train' 'es_train' 'es_train' 'es_train' 'en_train'\n",
            " 'en_train' 'et_train' 'es_train' 'et_train' 'et_train' 'es_train'\n",
            " 'fi_train' 'pt_train' 'es_train' 'pt_train' 'fi_train' 'pt_train'\n",
            " 'et_train' 'pt_train' 'en_train' 'en_train' 'en_train' 'es_train'\n",
            " 'en_train' 'et_train' 'fi_train' 'pt_train' 'es_train' 'en_train'\n",
            " 'en_train' 'et_train' 'es_train' 'es_train' 'pt_train' 'en_train'\n",
            " 'pt_train' 'pt_train' 'es_train' 'et_train' 'fi_train' 'fi_train'\n",
            " 'pt_train' 'et_train' 'fi_train' 'fi_train' 'fi_train' 'et_train'\n",
            " 'pt_train' 'en_train' 'en_train' 'es_train' 'en_train' 'fi_train'\n",
            " 'et_train' 'pt_train' 'en_train' 'es_train' 'pt_train' 'fi_train'\n",
            " 'en_train' 'pt_train' 'pt_train' 'en_train' 'en_train' 'fi_train'\n",
            " 'en_train' 'fi_train' 'et_train' 'es_train' 'pt_train' 'en_train'\n",
            " 'fi_train' 'en_train' 'en_train' 'fi_train' 'pt_train' 'en_train'\n",
            " 'pt_train' 'fi_train' 'pt_train' 'fi_train' 'es_train' 'pt_train'\n",
            " 'en_train' 'pt_train' 'et_train' 'et_train' 'en_train' 'es_train'\n",
            " 'es_train' 'et_train' 'et_train' 'et_train' 'pt_train' 'en_train'\n",
            " 'es_train' 'en_train' 'et_train' 'fi_train' 'et_train' 'fi_train'\n",
            " 'fi_train' 'en_train' 'es_train' 'fi_train' 'es_train' 'pt_train'\n",
            " 'es_train' 'en_train' 'es_train' 'en_train' 'es_train' 'fi_train'\n",
            " 'fi_train' 'es_train' 'es_train' 'fi_train' 'et_train' 'fi_train'\n",
            " 'fi_train' 'es_train' 'et_train' 'es_train' 'es_train' 'en_train'\n",
            " 'en_train' 'es_train' 'fi_train' 'es_train' 'fi_train' 'fi_train'\n",
            " 'et_train' 'es_train' 'fi_train' 'pt_train' 'fi_train' 'en_train'\n",
            " 'et_train' 'en_train' 'es_train' 'es_train' 'et_train' 'et_train'\n",
            " 'et_train' 'es_train' 'fi_train' 'pt_train' 'es_train' 'fi_train'\n",
            " 'en_train' 'et_train' 'pt_train' 'et_train' 'es_train' 'et_train'\n",
            " 'en_train' 'et_train' 'et_train' 'es_train' 'et_train' 'es_train'\n",
            " 'es_train' 'fi_train' 'fi_train' 'et_train' 'en_train' 'pt_train'\n",
            " 'en_train' 'en_train' 'es_train' 'en_train' 'fi_train' 'es_train'\n",
            " 'pt_train' 'pt_train' 'pt_train' 'fi_train' 'fi_train' 'et_train'\n",
            " 'es_train' 'fi_train' 'es_train' 'es_train' 'fi_train' 'en_train'\n",
            " 'en_train' 'en_train' 'en_train' 'es_train' 'pt_train' 'es_train'\n",
            " 'en_train' 'pt_train' 'en_train' 'es_train' 'es_train' 'pt_train'\n",
            " 'es_train' 'es_train' 'fi_train' 'et_train' 'fi_train' 'en_train'\n",
            " 'en_train' 'en_train' 'et_train' 'es_train' 'fi_train' 'fi_train'\n",
            " 'en_train' 'et_train' 'es_train' 'pt_train' 'et_train' 'et_train'\n",
            " 'fi_train' 'es_train' 'pt_train' 'fi_train' 'en_train' 'fi_train'\n",
            " 'es_train' 'pt_train' 'en_train' 'en_train' 'pt_train' 'fi_train'\n",
            " 'en_train' 'es_train' 'pt_train' 'es_train' 'pt_train' 'es_train'\n",
            " 'es_train' 'es_train' 'et_train' 'et_train' 'fi_train' 'et_train'\n",
            " 'es_train' 'en_train' 'en_train' 'pt_train' 'en_train' 'es_train'\n",
            " 'et_train' 'et_train' 'es_train' 'fi_train' 'es_train' 'pt_train'\n",
            " 'et_train' 'es_train' 'en_train' 'et_train' 'en_train' 'fi_train'\n",
            " 'fi_train' 'en_train' 'es_train' 'es_train' 'en_train' 'pt_train'\n",
            " 'fi_train' 'es_train' 'en_train' 'en_train' 'en_train' 'en_train'\n",
            " 'es_train' 'en_train' 'pt_train' 'es_train' 'en_train' 'es_train'\n",
            " 'es_train' 'en_train' 'et_train' 'et_train' 'en_train' 'fi_train'\n",
            " 'fi_train' 'et_train' 'pt_train' 'es_train' 'es_train' 'es_train'\n",
            " 'en_train' 'pt_train' 'fi_train' 'en_train' 'es_train' 'et_train'\n",
            " 'fi_train' 'es_train' 'fi_train' 'fi_train' 'en_train' 'pt_train'\n",
            " 'fi_train' 'et_train' 'es_train' 'en_train' 'es_train' 'fi_train'\n",
            " 'es_train' 'en_train' 'en_train' 'et_train' 'es_train' 'es_train'\n",
            " 'fi_train' 'fi_train' 'en_train' 'en_train' 'es_train' 'fi_train'\n",
            " 'en_train' 'et_train' 'et_train' 'en_train' 'es_train' 'es_train'\n",
            " 'et_train' 'et_train' 'en_train' 'fi_train' 'et_train' 'et_train'\n",
            " 'et_train' 'es_train' 'pt_train' 'et_train' 'et_train' 'pt_train'\n",
            " 'pt_train' 'es_train' 'fi_train' 'en_train' 'es_train' 'fi_train'\n",
            " 'en_train' 'pt_train' 'pt_train' 'fi_train' 'et_train' 'es_train'\n",
            " 'es_train' 'es_train' 'pt_train' 'fi_train' 'et_train' 'en_train'\n",
            " 'et_train' 'fi_train' 'en_train' 'en_train' 'fi_train' 'es_train'\n",
            " 'pt_train' 'pt_train' 'et_train' 'pt_train' 'fi_train' 'en_train'\n",
            " 'et_train' 'fi_train' 'fi_train' 'es_train' 'pt_train' 'es_train'\n",
            " 'fi_train' 'es_train' 'fi_train' 'fi_train' 'fi_train' 'et_train'\n",
            " 'et_train' 'et_train' 'es_train' 'fi_train' 'et_train' 'en_train'\n",
            " 'es_train' 'es_train' 'en_train' 'fi_train' 'fi_train' 'et_train'\n",
            " 'es_train' 'es_train' 'es_train' 'pt_train' 'pt_train' 'es_train'\n",
            " 'et_train' 'et_train' 'pt_train' 'fi_train' 'en_train' 'en_train'\n",
            " 'es_train' 'pt_train' 'pt_train' 'et_train' 'et_train' 'pt_train'\n",
            " 'es_train' 'et_train' 'es_train' 'en_train' 'pt_train' 'en_train'\n",
            " 'en_train' 'en_train' 'pt_train' 'en_train' 'pt_train' 'pt_train'\n",
            " 'pt_train' 'et_train' 'et_train' 'es_train' 'pt_train' 'en_train'\n",
            " 'en_train' 'pt_train' 'en_train' 'es_train' 'fi_train' 'fi_train'\n",
            " 'et_train' 'pt_train' 'pt_train' 'pt_train' 'fi_train' 'et_train'\n",
            " 'et_train' 'pt_train' 'es_train' 'fi_train' 'es_train' 'es_train'\n",
            " 'es_train' 'et_train' 'en_train' 'fi_train' 'fi_train' 'es_train'\n",
            " 'en_train' 'en_train' 'en_train' 'es_train' 'pt_train' 'et_train'\n",
            " 'en_train' 'fi_train' 'pt_train' 'fi_train' 'en_train' 'es_train'\n",
            " 'es_train' 'en_train' 'es_train' 'et_train' 'en_train' 'fi_train'\n",
            " 'es_train' 'fi_train' 'pt_train' 'pt_train' 'en_train' 'fi_train'\n",
            " 'fi_train' 'en_train' 'pt_train' 'en_train' 'pt_train' 'es_train'\n",
            " 'et_train' 'fi_train' 'fi_train' 'en_train' 'fi_train' 'en_train'\n",
            " 'en_train' 'et_train' 'en_train' 'es_train' 'pt_train' 'fi_train'\n",
            " 'et_train' 'pt_train' 'fi_train' 'fi_train' 'es_train' 'et_train'\n",
            " 'en_train' 'es_train' 'et_train' 'en_train' 'es_train' 'pt_train'\n",
            " 'pt_train' 'en_train' 'pt_train' 'fi_train' 'et_train' 'pt_train'\n",
            " 'et_train' 'fi_train' 'en_train' 'fi_train' 'et_train' 'et_train'\n",
            " 'fi_train' 'et_train' 'en_train' 'en_train' 'en_train' 'pt_train'\n",
            " 'en_train' 'en_train' 'en_train' 'fi_train' 'et_train' 'pt_train'\n",
            " 'en_train' 'es_train' 'fi_train' 'en_train' 'en_train' 'en_train'\n",
            " 'pt_train' 'et_train' 'en_train' 'es_train' 'es_train' 'et_train'\n",
            " 'et_train' 'es_train' 'pt_train' 'es_train' 'et_train' 'es_train'\n",
            " 'en_train' 'es_train' 'fi_train' 'en_train' 'es_train' 'et_train'\n",
            " 'es_train' 'fi_train' 'es_train' 'pt_train' 'en_train' 'pt_train'\n",
            " 'es_train' 'es_train' 'es_train' 'fi_train' 'et_train' 'fi_train'\n",
            " 'es_train' 'pt_train' 'es_train' 'en_train' 'es_train' 'en_train'\n",
            " 'et_train' 'pt_train' 'es_train' 'es_train' 'pt_train' 'pt_train'\n",
            " 'pt_train' 'et_train' 'pt_train' 'fi_train' 'en_train' 'es_train'\n",
            " 'pt_train' 'et_train' 'pt_train' 'pt_train' 'en_train' 'et_train'\n",
            " 'et_train' 'es_train' 'en_train' 'es_train' 'en_train' 'en_train'\n",
            " 'es_train' 'pt_train' 'fi_train' 'pt_train' 'en_train' 'es_train'\n",
            " 'pt_train' 'es_train' 'en_train' 'pt_train' 'en_train' 'en_train'\n",
            " 'es_train' 'fi_train' 'pt_train' 'pt_train' 'en_train' 'fi_train'\n",
            " 'fi_train' 'et_train' 'es_train' 'et_train' 'pt_train' 'et_train'\n",
            " 'et_train' 'fi_train' 'es_train' 'es_train' 'es_train' 'en_train'\n",
            " 'pt_train' 'pt_train' 'en_train' 'fi_train' 'es_train' 'es_train'\n",
            " 'fi_train' 'fi_train' 'et_train' 'fi_train' 'en_train' 'fi_train'\n",
            " 'pt_train' 'et_train' 'et_train' 'fi_train' 'es_train' 'en_train'\n",
            " 'et_train' 'et_train' 'pt_train' 'et_train' 'pt_train' 'es_train'\n",
            " 'fi_train' 'es_train' 'fi_train' 'es_train' 'et_train' 'et_train'\n",
            " 'et_train' 'es_train' 'es_train' 'en_train' 'es_train' 'fi_train'\n",
            " 'en_train' 'en_train' 'fi_train' 'es_train' 'es_train' 'en_train'\n",
            " 'es_train' 'en_train' 'en_train' 'et_train' 'en_train' 'en_train'\n",
            " 'et_train' 'fi_train' 'es_train' 'fi_train' 'en_train' 'en_train'\n",
            " 'en_train' 'fi_train' 'es_train' 'es_train' 'en_train' 'et_train'\n",
            " 'pt_train' 'es_train' 'et_train' 'en_train' 'fi_train' 'en_train'\n",
            " 'et_train' 'es_train' 'es_train' 'et_train' 'en_train' 'en_train'\n",
            " 'fi_train' 'es_train' 'en_train' 'es_train' 'fi_train' 'en_train'\n",
            " 'en_train' 'en_train' 'fi_train' 'pt_train' 'en_train' 'pt_train'\n",
            " 'pt_train' 'pt_train' 'en_train' 'fi_train' 'et_train' 'en_train'\n",
            " 'en_train' 'es_train' 'en_train' 'en_train' 'fi_train' 'es_train'\n",
            " 'et_train' 'fi_train' 'et_train' 'fi_train' 'fi_train' 'fi_train'\n",
            " 'et_train' 'en_train' 'es_train' 'fi_train' 'es_train' 'fi_train'\n",
            " 'et_train' 'en_train' 'fi_train' 'pt_train' 'pt_train' 'en_train'\n",
            " 'et_train' 'es_train' 'et_train' 'en_train' 'es_train' 'pt_train'\n",
            " 'et_train' 'fi_train' 'et_train' 'en_train' 'fi_train' 'et_train'\n",
            " 'en_train' 'et_train' 'es_train' 'fi_train' 'en_train' 'et_train'\n",
            " 'et_train' 'en_train' 'en_train' 'fi_train']\n",
            "[[168   0   0   6   0]\n",
            " [  6 218   0   0   2]\n",
            " [ 13   0 173  12   1]\n",
            " [ 20   0   4 161   0]\n",
            " [ 13   0   0   3 158]]\n",
            "0.9164926931106472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDdERxxxpHme"
      },
      "source": [
        "#20% of the dataset gives us a pretty good result of 91% accuracy, maybe by adjusting the C value and ngrams range we could get a even better result with a smaller dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT_GbBqXndl9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}