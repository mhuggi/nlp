{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract correct dataset\n",
    "from zipfile import ZipFile\n",
    "\n",
    "abst = ZipFile('abstracts.zip')\n",
    "\n",
    "for file in abst.namelist():\n",
    "    if file.startswith('abstracts/awards_2002/'):\n",
    "        abst.extract(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts abstracts from documents\n",
    "import os\n",
    "\n",
    "rootdir = 'abstracts/awards_2002'\n",
    "documents = []\n",
    "\n",
    "for sdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        with open(os.path.join(sdir,file), \"rt\", encoding=\"unicode_escape\") as f:\n",
    "            documents.append([line.strip() for line in f.readlines()])\n",
    "            \n",
    "abstracts = []\n",
    "for d in documents:\n",
    "    s = \"\"\n",
    "    for i, t in enumerate(d):\n",
    "        if \"Abstract\" in t:\n",
    "            for t in d[i+1:]:\n",
    "                s += t\n",
    "                \n",
    "    abstracts.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9923, 50000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set parameters and initialize\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, use_idf=True, sublinear_tf=True, max_df=0.8, max_features=50000)\n",
    "\n",
    "# Calcualate term-document matrix with tf-idf scores\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(abstracts)\n",
    "\n",
    "# Check matrix shape\n",
    "tfidf_matrix.toarray().shape # N_docs x N_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '0001', '000and', '000events', '000front', '000l', '000species', '000to']\n",
      "['zr', 'zro2', 'zuni', 'zurich', 'zworski', 'zygmund', 'zygmundoperators', 'zygomycetes', 'zygomycota', 'zygotic']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "print(tfidf_vectorizer.get_feature_names()[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[:,tfidf_vectorizer.get_feature_names().index('record')].toarray() # Get doc vector for term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7238\tbe\n",
      "6988\ton\n",
      "6814\twith\n",
      "6758\tthat\n",
      "6220\tare\n",
      "6195\tresearch\n",
      "6115\tas\n",
      "6101\tby\n",
      "5505\tfrom\n",
      "5160\tan\n",
      "5022\tproject\n",
      "5006\tat\n",
      "4810\tthese\n",
      "3836\thave\n",
      "3734\twhich\n",
      "3543\tit\n",
      "3509\tnew\n",
      "3427\tor\n",
      "3293\thas\n",
      "3283\tsuch\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "terms_in_docs = tfidf_vectorizer.inverse_transform(tfidf_matrix)\n",
    "token_counter = Counter()\n",
    "for terms in terms_in_docs:\n",
    "    token_counter.update(terms)\n",
    "\n",
    "for term, count in token_counter.most_common(20):\n",
    "    print(\"%d\\t%s\" % (count, term))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 0, top terms by TF-IDF\n",
      "0.28\tchow\n",
      "0.16\thodge\n",
      "0.15\talgebraic\n",
      "0.14\tsubgroup\n",
      "0.14\tabelian\n",
      "\n",
      "Document 1, top terms by TF-IDF\n",
      "0.24\tcareersin\n",
      "0.24\tfriendships\n",
      "0.22\tcultivating\n",
      "0.22\tinterestto\n",
      "0.21\tethnically\n",
      "\n",
      "Document 2, top terms by TF-IDF\n",
      "0.24\tupdating\n",
      "0.16\tanddamped\n",
      "0.16\tsimulationmodels\n",
      "0.15\tsecondly\n",
      "0.14\tinvolveundergraduates\n",
      "\n",
      "Document 3, top terms by TF-IDF\n",
      "0.18\ttheconference\n",
      "0.17\tcomputations\n",
      "0.16\tconference\n",
      "0.16\talgebraicobjects\n",
      "0.16\tandcomputerscientists\n",
      "\n",
      "Document 4, top terms by TF-IDF\n",
      "0.18\tpreferences\n",
      "0.17\tmarketing\n",
      "0.16\tselection\n",
      "0.16\trobust\n",
      "0.15\tproduct\n"
     ]
    }
   ],
   "source": [
    "features = tfidf_vectorizer.get_feature_names()\n",
    "for doc_i in range(5):\n",
    "    print(\"\\nDocument %d, top terms by TF-IDF\" % doc_i)\n",
    "    for term, score in sorted(list(zip(features,tfidf_matrix.toarray()[doc_i])), key=lambda x:-x[1])[:5]:\n",
    "        print(\"%.2f\\t%s\" % (score, term))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector length: 50000\n",
      "Non-zero dimensions for document 0: 185\n",
      "Non-zero dimensions for document 1: 60\n",
      "Non-zero dimensions for document 2: 124\n",
      "Non-zero dimensions for document 3: 98\n",
      "Non-zero dimensions for document 4: 137\n"
     ]
    }
   ],
   "source": [
    "print(\"Document vector length:\", tfidf_matrix.shape[1])\n",
    "for i in range(5):\n",
    "    print(\"Non-zero dimensions for document %d: %d\" % (i, len([x for x in tfidf_matrix.toarray()[i] if x > 0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word: accompany\n",
      "Occurs in 109 documents\n",
      "out of 9923 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample word:\", features[1000])\n",
    "print(\"Occurs in %d documents\" % len([x for x in tfidf_matrix.toarray()[:][1000] if x > 0]))\n",
    "print(\"out of %d documents\" % len(tfidf_matrix.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=30, random_state=123)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_sample = tfidf_matrix[:1000]\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Do clustering\n",
    "km = KMeans(n_clusters=30, random_state=123, verbose=0)\n",
    "km.fit(matrix_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq, numpy as np\n",
    "\n",
    "# Custom function to print top keywords for each cluster\n",
    "def print_clusters(matrix, clusters, n_keywords=10):\n",
    "    for cluster in range(min(clusters), max(clusters)+1):\n",
    "        cluster_docs = [i for i, c in enumerate(clusters) if c == cluster]\n",
    "        print(\"Cluster: %d (%d docs)\" % (cluster, len(cluster_docs)))\n",
    "        \n",
    "        # Keep scores for top n terms\n",
    "        new_matrix = np.zeros((len(cluster_docs), matrix.shape[1]))\n",
    "        for cluster_i, doc_vec in enumerate(matrix[cluster_docs].toarray()):\n",
    "            for idx, score in heapq.nlargest(n_keywords, enumerate(doc_vec), key=lambda x:x[1]):\n",
    "                new_matrix[cluster_i][idx] = score\n",
    "\n",
    "        # Aggregate scores for kept top terms\n",
    "        keywords = heapq.nlargest(n_keywords, zip(new_matrix.sum(axis=0), features))\n",
    "        print(', '.join([w for s,w in keywords]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 27, 16, 19, 22, 19, 19, 28, 29, 22, 23, 19, 18, 28, 27,  5, 19,\n",
       "        5,  5, 29,  2,  0,  2,  2, 16,  2, 14, 19, 22, 22, 16,  2, 22, 16,\n",
       "        7,  0,  7,  7, 22,  5, 23, 25,  2,  5,  2, 19,  7,  5, 16, 23,  7,\n",
       "        2,  7, 11,  5,  0, 16,  7, 22,  1, 23, 22, 11, 26,  1, 19,  7,  2,\n",
       "        7, 22,  0,  0,  0, 23,  7,  5, 22,  0,  5,  5,  1, 22,  1,  0,  0,\n",
       "       22, 25,  0,  2,  5, 22, 22,  7,  5,  7, 22,  2,  5, 23,  3, 16, 16,\n",
       "       16,  5,  0, 22, 25, 25, 23, 28, 19,  2,  5, 19,  2, 14,  2, 27, 16,\n",
       "        5,  2,  5, 14, 16,  8,  2,  5,  5, 28,  2, 22, 28, 23, 19, 11, 29,\n",
       "        7,  5, 22,  7,  0,  7,  7, 25, 19, 16, 19, 19, 11, 25, 23,  9, 23,\n",
       "        7,  2, 12,  7,  7, 25, 23, 22, 25,  7, 19, 19, 18, 14, 11,  7,  7,\n",
       "       25,  7,  1, 19,  7,  7,  7, 25,  2,  2, 19, 23,  7, 19, 19, 19, 25,\n",
       "        3,  0, 28, 28,  7,  7,  7, 19,  7, 28, 25,  1,  7, 11, 18,  7,  5,\n",
       "       27, 27,  7, 25, 19, 22, 29, 19, 10, 19, 29,  0,  0, 19, 10, 15, 19,\n",
       "       23,  7,  0, 20,  5, 23,  7, 19, 23, 23,  7, 19, 23, 22, 28, 27, 23,\n",
       "        5,  7,  0, 16,  0, 16, 29, 22, 12, 23, 25,  3, 24,  7,  0,  3,  1,\n",
       "        3, 16, 28, 16, 27,  7, 27,  0, 26,  7, 21, 19, 19,  5, 16, 16, 10,\n",
       "       15,  0,  7, 10, 23, 28,  9, 28, 16, 28, 16,  5, 16,  0, 29, 16,  0,\n",
       "       16,  9, 23, 15, 25, 28,  5, 19,  5,  0, 14, 16, 25, 16, 22,  5, 16,\n",
       "       22, 22, 16, 11, 22,  3, 16, 16, 11, 28, 27, 20, 13, 22,  5,  0,  9,\n",
       "       16,  7, 22, 14,  0, 23,  3, 22,  1, 16, 29,  7,  5,  5, 11, 29, 22,\n",
       "       27,  5,  5,  5, 20, 16, 24, 19, 28,  5, 26,  9, 20,  9,  0, 22,  0,\n",
       "        0, 22,  5, 19, 27, 16,  5, 22,  5,  0, 13,  7,  7,  4,  5, 28,  0,\n",
       "       15, 22, 27, 10,  9, 13,  9, 13, 13, 28, 13, 28, 19, 13, 12, 12, 19,\n",
       "       22, 11, 12, 19, 25, 27, 10, 18,  0,  8, 21, 27,  5, 10, 14,  0,  0,\n",
       "       18,  5, 22, 22,  0, 28,  0,  8,  8, 15,  0, 28,  5, 27,  5, 19, 28,\n",
       "        1, 28, 22,  9,  0, 20, 18, 28,  5,  8, 21,  3,  5, 25, 22, 28,  1,\n",
       "       27, 23,  5,  5, 22, 28, 14, 28,  3, 25, 25, 12, 18, 27, 25,  5, 15,\n",
       "       13, 29, 28, 28,  8, 29, 28, 15, 28, 15, 28, 28,  5, 28, 28,  5, 28,\n",
       "       26, 20,  0, 28, 28,  7, 15, 26,  0, 18,  5,  5,  3,  3,  7, 28, 28,\n",
       "       16, 12,  5, 20,  3, 28, 27, 28, 28, 27, 15, 15,  0, 27, 28,  7, 28,\n",
       "       21,  6, 21,  0,  7,  8,  7, 27, 27, 28, 26,  0, 14, 21,  8, 14, 23,\n",
       "       20, 14,  3, 22,  8, 12,  0,  8,  0, 21, 12, 21,  9, 23, 15, 21,  8,\n",
       "       21, 21, 29,  8, 15, 11,  8, 21,  8,  4,  8, 21, 21, 16,  8,  3, 19,\n",
       "        8, 14, 27, 15, 28, 26, 10, 21,  7, 21,  8, 21,  8,  8, 23, 13,  8,\n",
       "        8, 28,  8, 12, 12, 12, 12,  8,  4,  8,  1,  8,  8,  8, 21, 18, 23,\n",
       "        8,  8, 27,  8,  8, 21, 21, 21, 21,  4,  6, 21,  8,  4, 21, 21,  8,\n",
       "        4,  8,  7, 14,  4,  9, 21, 13,  4,  4,  4,  4, 12,  0, 28, 18,  0,\n",
       "       19, 13,  4, 21,  7,  4, 22,  4,  7,  4,  7,  6,  4,  4, 13,  8, 13,\n",
       "        7,  0, 15,  7, 12,  4,  4, 28,  4,  4,  4,  4,  7,  6, 15,  7,  4,\n",
       "        4,  8,  4,  6,  6, 20, 27, 27, 28,  6,  6, 25,  6, 28, 27,  6,  6,\n",
       "       22,  6, 18,  6,  9,  6,  6,  7, 20, 19,  6,  6, 28, 20, 14,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6, 14, 28,  6,  6, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  0,\n",
       "       15, 24,  7,  9,  6, 12, 15,  0, 22,  0, 27,  7, 28,  7,  8, 27,  7,\n",
       "       28,  0, 19,  7,  8, 15,  8,  8, 20, 16,  8, 14,  7,  0, 28, 24,  3,\n",
       "        8, 12,  5,  7,  7,  8,  8,  8,  7, 28, 11, 19,  8, 27,  7,  0,  7,\n",
       "       28,  7, 12, 12,  5, 19,  3,  7, 27, 16, 20,  1, 22, 14,  0,  7, 16,\n",
       "       19, 27, 27, 28,  0,  9, 23, 28, 27,  0,  0,  0, 10,  5, 22, 20,  0,\n",
       "       19, 27, 25,  0,  3, 16,  0, 27, 18,  0, 22,  7,  0, 19,  7,  6, 27,\n",
       "        0,  9,  8, 19, 16, 12, 25,  0, 16, 28, 19,  5, 27,  7, 26,  1,  0,\n",
       "       26,  3,  7,  0,  0, 10, 18, 26, 16, 14, 22, 28, 28, 28,  7, 22, 18,\n",
       "        5, 23, 24,  7, 17,  7,  3, 16,  8,  8, 18, 20,  0, 17,  0, 16, 27,\n",
       "       16, 24, 19,  0, 18,  5, 17,  7,  9,  0,  0, 22, 18,  7,  8,  0,  0,\n",
       "       14,  9,  5, 17, 19, 17, 22, 14,  7, 16, 17, 16, 17, 14, 14,  5, 17,\n",
       "        7, 19, 28, 28, 14, 29, 17,  0,  5,  3, 22,  0,  7, 10, 20,  5, 29,\n",
       "       23,  0,  8, 24,  7,  5, 27, 11,  1,  5,  7, 10, 16,  0, 29, 27, 28,\n",
       "        7,  7,  7,  7, 16,  0,  0, 20, 14, 28, 16,  0, 25, 23, 28,  7, 19,\n",
       "        7, 12,  7, 22, 16, 28, 16, 18, 16,  7,  0, 23,  0,  0, 23, 28, 16,\n",
       "       16, 18,  0, 18, 10, 18, 16, 26,  0,  5, 10, 23,  5, 24])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 (88 docs)\n",
      "magnetic, films, quantum, spin, polymer, crystals, superconductors, polarized, superconducting, assembly\n",
      "\n",
      "Cluster: 1 (14 docs)\n",
      "algebras, conformal, graphs, representation, lie, representationtheory, percolation, symmetric, magma, symmetries\n",
      "\n",
      "Cluster: 2 (20 docs)\n",
      "fellowship, training, biology, microbial, fungal, bacterial, bacteria, virulence, foreignlocations, endophytes\n",
      "\n",
      "Cluster: 3 (20 docs)\n",
      "abstract, minkowski, required, resistivity, no, 3d, cardiac, sinkholes, cave, resultants\n",
      "\n",
      "Cluster: 4 (26 docs)\n",
      "fellowship, mathematical, sciences, fellowships, zygotic, zygomycota, zygomycetes, zygmundoperators, zygmund, zworski\n",
      "\n",
      "Cluster: 5 (67 docs)\n",
      "fiber, damage, straightening, repair, grid, polymer, laminar, ultrabroadband, cad, centrifuge\n",
      "\n",
      "Cluster: 6 (30 docs)\n",
      "twenty, dr, archaeological, exotic, co2, amb, native, cells, healing, advective\n",
      "\n",
      "Cluster: 7 (97 docs)\n",
      "equations, manifolds, operators, curvature, equation, harmonic, fluid, einstein, dynamical, hyperbolic\n",
      "\n",
      "Cluster: 8 (52 docs)\n",
      "teachers, mathematics, teacher, college, teaching, modules, school, instructional, workplace, curriculum\n",
      "\n",
      "Cluster: 9 (17 docs)\n",
      "terrorist, attacks, attitudes, wtc, voting, trust, speckle, public, accessibility, coordinating\n",
      "\n",
      "Cluster: 10 (14 docs)\n",
      "energetic, particles, mhd, magnetohydrodynamic, solar, alfven, pulse, waves, ligo, plasma\n",
      "\n",
      "Cluster: 11 (13 docs)\n",
      "arithmetic, elliptic, functions, curves, curve, conjecture, dyer, birch, tangent, zeta\n",
      "\n",
      "Cluster: 12 (43 docs)\n",
      "available, not, zygotic, zygomycota, zygomycetes, zygmundoperators, zygmund, zworski, zurich, zuni\n",
      "\n",
      "Cluster: 13 (13 docs)\n",
      "fellowship, postdoctoral, he, galaxies, andastrophysics, redshift, she, her, astro, slit\n",
      "\n",
      "Cluster: 14 (24 docs)\n",
      "museum, yeara, umebproject, progressmonitored, ofbroadening, breaksand, biology, byengaging, naturalhistory, academicyear\n",
      "\n",
      "Cluster: 15 (19 docs)\n",
      "particle, magnetic, magnetosphere, ring, ions, particlesin, relativistic, compounds, ringcurrent, acceleration\n",
      "\n",
      "Cluster: 16 (56 docs)\n",
      "regression, mantle, seismologists, rna, graph, procedures, wedge, nigms, financial, motion\n",
      "\n",
      "Cluster: 17 (9 docs)\n",
      "crcd, curriculumdevelopment, dame, notre, underthe, pittsburgh, entitled, communications, ut, ethics\n",
      "\n",
      "Cluster: 18 (21 docs)\n",
      "workshop, kdi, aerosol, gaim, ices, texts, carbon, indoex, africa, beirut\n",
      "\n",
      "Cluster: 19 (54 docs)\n",
      "algebraic, algebras, curves, finite, spaces, sheaves, manifolds, topological, topology, algebra\n",
      "\n",
      "Cluster: 20 (17 docs)\n",
      "iron, corrosion, clay, phosphorus, ofreduced, reduced, shales, srb, shale, soils\n",
      "\n",
      "Cluster: 21 (27 docs)\n",
      "colleges, college, manufacturing, industry, technicians, standards, xenon, module, curriculum, technical\n",
      "\n",
      "Cluster: 22 (53 docs)\n",
      "sdp, control, autonomous, nonlinear, linear, printing, nlp, power, matrix, manufacturing\n",
      "\n",
      "Cluster: 23 (34 docs)\n",
      "algebra, algebraic, varieties, commutative, geometry, he, ideals, cohomology, matrices, polynomial\n",
      "\n",
      "Cluster: 24 (8 docs)\n",
      "sediment, floodplain, clinoform, strickland, fly, trapping, thestrickland, floodplains, proportion, damping\n",
      "\n",
      "Cluster: 25 (25 docs)\n",
      "algebras, quantum, operator, subfactors, arsenic, lie, mobilization, spaces, iron, von\n",
      "\n",
      "Cluster: 26 (11 docs)\n",
      "mesoscale, lake, mineralogy, clay, friction, subduction, shear, kona, theup, cmes\n",
      "\n",
      "Cluster: 27 (41 docs)\n",
      "conference, gulf, seminar, symposium, pbo, meeting, rifts, basement, meetings, beingcarried\n",
      "\n",
      "Cluster: 28 (72 docs)\n",
      "contract, china, fellowship, climate, tumors, adolescents, figurines, social, anthracis, biological\n",
      "\n",
      "Cluster: 29 (15 docs)\n",
      "oceanographic, vessel, ship, ships, equipment, operated, nationaloceanographic, researchfleet, vessels, profiler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_clusters(matrix_sample, km.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADtCAYAAACvfY5sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeg0lEQVR4nO3dX4xc1X0H8O9v/3g3xnYIWRxcQ2LT0garDVHqQvMQNVHUFqJWVqpKhkSNgpoYqlC1D5VC+1CE8kCi9KGKQmKcCKJIobhSo0JbAxWRnFQEXIxkm7AbItcGe/E668Ux+4+Z3Zk5fdj5zZ45e+69587c2Zk5+/1ICO/MnZkz5577veeee+4dMcaAiIj630C3C0BERMVgoBMRRYKBTkQUCQY6EVEkGOhERJEY6tYHj42NmV27dnXr44mI+tLLL788Y4y51vdc1wJ9165dOH78eLc+noioL4nIG0nPcciFiCgSDHQiokgw0ImIIsFAJyKKBAOdiCgSDHQiokhkBrqIPCoi0yLys4TnRUS+ISKnReSUiHyk+GISEVGWkB769wDcnvL8HQBuqv93AMC32y8WERHllXlhkTHmJyKyK2WRfQC+b1ZurP6iiFwtIjuMMVNp73t5YQn7H3kBAHDy/BUsV2vYPLJSnLEtI9i+dSS1XNNzZczMl7OK32SxXMm1PAAMDw5geKj1kalWPtNWM0DeO9YPSr7l9Tvu2bEtcRm3vu3v1W4d5bFcqWG5WgOARnvJo5314fuedp3ZdaSfk1Y39ncBVr5PnnrV19v1EPL65UptzeNuWWyh9ex731b52mLebT5t3bh145Y9qT7s1/nq3xVSJyHb3d/fcTM+c9v7U98HKOZK0Z0Azlt/T9YfWxPoInIAK714vOu6X8exs5exe+wqLFdrqJqVL1+q1BqN0g51X6BUTXN45QmW0KCs1suUJi1AqwYQAAMZIdtKcKe9V9rnuZ+l3/H465exeWTIu0OdmS9jsVxpNF77Par1hp2n/tMCJE3VKnjecB4ezB827vcsVWoYlLVtbXqujHNvLaBqgKEBaZQzrW603dvtx1evWvak1zf+rjT/nfT65eraIPKVJS/f+xbJbYOtvrZqAFRXglbbYdUApUoNo0Mr9eTWh66Xaj0L7GXStFsnM/NlzJUqePLEm+sW6L4m4P2axphDAA4BwDUfuNkAK6Fth8f41Cz08cP3fLTx+P5HXsDMfDlxrztXqmCz1cvc9+GdiRXw+LFzeOjpCcyVKtg6uhpgre40ksqk5doyOtS0jFu2tPJoSPhCZGzLyJoei+/z3M/c/8gLGJ+abfRUtGEODw5grrS6M9XXPH7sHManZrF5ZKjRU9D30M/Rf9vrTL/bkyfeXLMOxqdmUarUsHXU3wRDjtJsWb03bR+33HB14rK+OnvyxJuNNmmHiftdtX2630E3SG2bdp0+9PQEADT1vux61XKGvt5dJ1ovZ2cWsHlooGk5+/sBq9tdyPaTRN/XbQNFafX97fb7yQ++Dz/6+S8BoKmOx7aM4OzMAnZc/S588WM3NupW69xuB7ruQ8rTbp3sf+QFHDt7OXj5IgJ9EsAN1t/XA7iQ9aLl2sqe7vSleSzV93pjW0a8hx+6QoC1waQhrytmfGoWc6UKTp6/0rRS7NdqI75t9zUYn5pt7Dx8Ow0A3g3LLYsGly6nQbV77KqmncVcqYLxqVk8eeLN4PJoY3M36qTyurTu7A10bMsIZrDaq7tt9zWNZffs2Nb0GjuQ7Z6CHWracF26IbhlTKpjAE07FbeeXW69a32nCenp6fd3y7rvwzvx0NMTGJ+axf5HXmgqm7tz08+y6xZIr1N9H3v97x67CjPzZRw7e7nRdqbnVuvqq/V2bpfZbSO+z7HXjf0dfe2ln9l1bYf5+NTsmiGR7VtHGsvb2wGwuoMen5rF48fOrVPp8yki0J8CcJ+IPAHgNgBvZ42fA0ClfqyyZA1naMPT/z9+7FxQ49eNTDdoXRH2Rus2Un3d+NQspufKTTsNwL/j0A3Tfj8tk+5I9PO1ZwQAX/zYjY3QuW33NZieW9043e8xPjXb1GDsYHCDVp9P2vvbvWM7gPT9dMdjDym5vQ/78SR23ek6c1+b1UNxd07Tc6s7Z3cHaLNDyd4ZJtWD2jySfNSk32d8anbNkYK+z9iWERw7exnTc+XU4NPvbr+nu27TpO0Y9uzYhmNnL2O2fkiuj9ntanxqFpsGB7BUrXmDyF43vvbi1k2/8h1J6hGNdiD0ee1Y2tujezTsO+rsBZmBLiL/AuDjAMZEZBLAAwCGAcAYcxDAEQCfAnAawCKAu/MWwq5st9K0Ibl7R7enAQBbR4eaercA1mxMf/vEiUaDn55bCbU9O7at2VDdjQLAmiGI6blyYqC4PSP9nknPpf1bX5u0A3J7ivZ7uMMiIfS9x7b4e7pugD954k0sV2soLdfwnf85k2vj19e7O1N7B2iX3ffevp1hUk9Ul7e5723Xvd3J0HDX15+dWQgeFkpbtyHcoyHfunTrQdu3PayV9tm+oym3bpKG0Oxlk47Wen3HsHV0qOno0NcONIfskQVtb7668dVJJ+shZJbLXRnPGwBfKqxE8PdcfHtHuwdlB74d2m4l6yGX+5725/rCVv/WIwbdEejrDt/zUXzin44m9sB0SCGrd5Y0FDG2ZaTpc5N2QGm94/2PvIDp2RJmFpaw7Jzo1cf1M+3vp49NXXkHw0MDjcN9u46GBwdQWq5h+9aRNT09/eysnY4euUzPlRvBmTWk45ZD/53nKMH33m4nQ+vDXh9Kd3DaE35nqert3acNM2m7sNutj36Wu/589WC375CjAV3OPTfgvr9vCM33Ge4wmh5pAb0X7vYJ9sVypakNAmjUuZ7TcneSmgtu3di5BKytB1VUfXTtfuih7AZ8yw1XNx7XcFuq1DB15R0AaIyF2aFt946BlSmSLnfjcxtm0tiZHYKPHzuH7VtXxrr1M+0GPbZlpHG22qbv4W78dvCWKrXUDShpjNRnZmHJO9Qys7DU6GX7wmJmvoySNeOnMTTi2UHYDVvrzh7/dRuvfeSSp9ebVB95ZA0XuXQd27Qtag/P7eklSTuE/85P/q/RtnRHoTtKt9ftyntEllfIEBrgn8hgl6uXAr1pdpDxr7fhoYE1w5Mue4jNPadmnyPs1M6uJwNde4PL1VqjtzgowMnJKxgeHMDYlhG8NV/GoADlegVrhc3Ml9eEv76nVmDJes3w0MCajc8N4qQNTzc49/HG89br3KMB/Qw3sN3H3eD1CQmypJ6d9kz0fENpubam4So7qBrfK2EH4Q4vhQydZJUdSO7pA81HM6FHCbrMXKmCh56ewP/WdzzaA9eZQPo+7jBU2tDUcqV53NouX1LnwRfGS9WatzOQxG5DPhNTs1iqT6UEkndkeXd2Pr7wzzraWg9ZRzq26blyUzsAVtftcrWGTZ7psL7hTvv8Tt6dnW8o0acnAl0b4Hy5ggFgZfZFzaBqVoOwaoDq8koPcs+ObWt6SYvlCs7VpxnavRe3pxsSkL4gDh0iSXpeV5R9gsXbU0543A6Ok+evYHhowDvdUsPDHpd2e3b2Dk0DOms+bSj3qEXrIOnEYFIg2kNUvhN+2rDtkLSHRnwzOHwbiw4f7R67CmdnFvCjn/+yabjp7MwCUK15d2a+x2zaWdDedtrRlk9aT1sDxcfXhux6mi2tXvQErBwNuO0lazJCv/NtDy67znTuurI7gqVl/+uzhgzTTkjb4a3tLGQd9MTNubQBGrMaLJs3DeZ6j6rxh5L2IH0XvCyWK409tI6bKXcqF7A680KX09fYj7n0+eXKaij45kBPz5YSewzu60qVlR6b9vLcIwi3d2BvmO1euZol7ajFDQjdqOxzC2r71uYhKm387jrR5ZLmlR++56ONmT3A6uwN/by01xclrQ3mMWEFuB0oABLbX6MM9W3MHdPfs2MbZhaWEtuL7mx7eapelum5Mk6ev7JyRXpAj1yltQ13O7InSuhnap35OjKutG1WyxyyDroW6DWT3CXMCp12xgb1vQdlZQegV9LpuNnE1GxTUNsNwF3BegThbij2itXnszbmdjd6dwekYTa2ZaRRnq2jQ42d3mK50rFw9+0Mk56zex8qbQfZCre37jsppew60TLUnKsxAWC+VMkMh6Lrd9ZzNKAXvaXtkELWtW+HqRe26TbSq1P1smzfOtJ0/qeI9uV2HrVzojlgd0hCZzgldVqWq7XGes5aBz0x5OJKO/zPOsxNE/Ja3Wg0iEeHBrBYrqSOldpG6z0nHT7KuuS3qI3eNz4L+E/iAWFDLN3olbWzfm32kI1uvDp+7s5GAlbXg10vGpL2bRQ03AxWT5LZ5yFsbh3r86FtKUTIerSXydMZ0razeTD9Pj/9xD2yKVrSNtNu/bnXTSTpiSGXoi2WK5iYmm30ttth32/EvfGS+297eR0+sjd2X48ua4NMep3LPlKwD/fysL+L23PO6uW5z2UNRbna7TXZ5bOniD154s01wzfu38Da9eAenem/a5711bhvS8a61Ofto7zMIbvZUq42rCfwfIraWW4Uuk5Ch2l8R5v2exV95OkTZaBXzUpPe7ZUCd7I0rh7R3eKE5AcSPbGnnc4RYdIdCZPaMPyBVaItLrwnaPQHaYOX/nK4D1f4DTutHML9mv0ghrfzkrLF/JeWfQ97JNluqPOusmaT1LALpYrOPfWwpohOzU9V8a5y4u5Tlhv3zrS1gnuVjsDMdI2FLLdZnUefUOznRBloK83DYCpK++kbkzLVjCnbeQufUv37oRJJ1LtEzOdoOcYQnaYyj5p1MqJSL0OwHfSyC1br/HVkd74La3+ZubLbYVzK3XRamegXyXV0USOoSkATdtCO9tdyAnUNH0X6PMJKyDrtp+dPAmosm4Hq8/7bru5WK40nbT0cZ/znUgdn5ptXGhVRG/A17CTLrwAVqfTuY3aPWmUxbeukk4auWVL4m4koYfAScHbTnsqappoO5+hvXG9sjUpQJLWaQyS6mi21HpeJE2QCNHuLSK6flJ0PmFMz55OaEuaHONbMe3Ms3bHT0MvQkiTNtZaxAau37ede1q7fDMr0hR10qnd+tB1NTE1i5t3bGvUjc4JH22znL5hJtVKEGg5QyVtN3noMMCgYE2v3D5xu1zNN3/eJ+lCL6D3bgOg3PWbt022es7CnWmkvxURoquBnrZR6FSd9ejJ2Nzx06pZ/WGDkIuS1pM7u6KoHxZwr4rrNHsoyh2v1Jsg6TLu1YtJdAbKbP2yan190pWtRbLbbGgbDt1xjk/NYjThSt5WueVzg6iIbTDWW/V2IqPsCwXtIA+ZHdX1HnqS9Q7yIhRZ5pBeXujsiryqBthccGikafyqjufk01L9Iiq9+jf0kNSuP1+A9+JYe4gid0brXQe9cBuA5ZwTDIr6rDwTIuxzS/YwbchRUt+NoW8UWSfMsl7r6laA+cZfk6aDhZ7nSLtwSYXOburXYC9C3valJ+FDTti5V072irTOQ6c+C2huZyEnPkPauM+GDfR+PALISzdAALnG4ULYF6ik1aWOqfvuj+NuVGk7MfcKTt0g8gZG1sU/Wdaj3RRx/UQnZN2Mzpb3JHi/CmkPw0MDTcv5jjKLmqfes0Mu/aSIE1SdoOPFqpwj0LMCJetkT2gguT9snMR3BSeAxoyeUEUFsu8CoyxFjad3KuxDxmhjuWJ0PXeYvo6U755Eviu68+pqD72VjaIXFf01kmaptDt7Jc9FMa1eLu57vc2dLZQ1Fpy04Wnw5NlJFWk9mq7vu2dNbW2Vngg999ZC6pGcfXTUzxcf5anDvEe2bk87azpzkboa6L2W50VMTWxH2uXleV5ftKTeeN4djIaGNvA8J36B5jtX6pS70J1Unh5Zp3pvIfPC8yzfCVlXNHfrdzW7OSafJ5B1CmjSRXNZ30HPI2lbz5sFG3YM3ccOG5/1ujCplcvL7dcXpZUjgjx1lDew9DJ5X+BklTXPZ3Xr/ErIlbPvLFVbfv+i2m+rJ+za0c9j8tqessJel7Xv4upmQdbRctRj6K004DxXahYthhO1SXcYLCJM+rF+ii5zpY1xynYurgM29oygovl+mzZLyIVKUffQ+zEA0uT9Pu3O6CiCPVc+tvURO/f8hr0O++kmXvYQR6+0QfdmXe4wjO5M85Y36kDfyLpxlS1tHP10E69O/hpVUdwyDg8NpP4QeBIGekQY4LTe+qWXvl50RKzoCQo1EzbkxUCnddfujodjuZ0VWr96g7OHnp5gqNdp0/aduLfDPs/vhAIrEyVCthsGOiXq1R5/r5arF+S9j7dPaP3m+a1Lag77TtUdA52oR+j843bkvd1xu/SunBx6CadXRw/Xf6s1788MpmGgU8vYUy5W2g+H9JLpuTJOnr/S9POIAHvptsVyJfGiIB2O0etGZhaWCtuWGOiRiuW2CrQ+8pzEm5kvN6Y0DsjKhUZjW0a8vfQihoD6UdW0foEg0Pp5IgZ6Dypib91reV7kryjZtK64A2tPu1cZJ01jXO8hoH5SM8m3G2n17qgMdIoC87w97d6sSsf+OZYezgCJt7IAWtvJRn3pP8WLUxe7x76C1O5lcsZL9zHQaV104mfyqPsG6jecGpSV37SN5X7p/SpoyEVEbheR10TktIjc73n+3SLyHyJyUkReFZG7iy8qEVH/6EanI7OHLiKDAB4G8IcAJgG8JCJPGWPGrcW+BGDcGPOnInItgNdE5AfGmCXPWxJRJGrWSWn9/VgAue4iuNGE/sJZKzuEkB76rQBOG2PO1AP6CQD7nGUMgK0iIgC2ALgMgIOcRJHTzBmQ1ZN4+stH5NfJjntIoO8EcN76e7L+mO2bAG4GcAHAKwD+xhiz5hStiBwQkeMicrzF8hJFrZWf++sFemtdHUPnOY5ihU7LDQl03wxi9+3/GMAJAL8G4MMAvikia86OGGMOGWP2GmP2hhWPaOPQX7TZKPRn5canZrH/kRc27HTHkBlbofvHkECfBHCD9ff1WOmJ2+4G8EOz4jSAswA+GFgGItqAdHrjnh3bMD41uyGnOxb9uwUhgf4SgJtEZLeIbAJwJ4CnnGXOAfgkAIjI+wD8FoAzxRWTiPpVWs97z45tOHzPRzfsdMeih6YyZ7kYYyoich+AZwEMAnjUGPOqiNxbf/4ggK8A+J6IvIKVIZovG2Nmii0qESUp4k6NnbIRe97dEnRhkTHmCIAjzmMHrX9fAPBHxRaNiELpL8V328nzVyDgrRi6hfdyIYpAr5xQLVVqa8K8V48cYsRAJ6KO6od7vKfppymYDHQiokgw0ImIIsFAJ4oAf+CDAAY6URSY5wQw0ImIosFAJ+oR/TSbgnoTA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA536EO9iReTDQCciigQDnYg66q3J+W4XYcNgoBNRx4gBPn1psNvF2DAY6EREkWCgExFFgoFORBQJBjoRUSQY6EREkWCgExFFgoFORBQJBjoRUSSCAl1EbheR10TktIjcn7DMx0XkhIi8KiI/LraYRESUZShrAREZBPAwgD8EMAngJRF5yhgzbi1zNYBvAbjdGHNORLZ3qsBEROQX0kO/FcBpY8wZY8wSgCcA7HOW+QyAHxpjzgGAMWa62GISEVGWkEDfCeC89fdk/THbbwJ4j4gcFZGXReRzRRWQiIjCZA65ABDPY+4NqYcA/C6ATwJ4F4AXRORFY8wvmt5I5ACAAwCw6brfyF9aIiJKFNJDnwRwg/X39QAueJZ5xhizYIyZAfATALe4b2SMOWSM2WuM2dtqgYmonxiU5/4Vl944g0tvnMHhB+/Hqeee6XahohUS6C8BuElEdovIJgB3AnjKWeZJAB8TkSER2QzgNgATxRaViPqPgalewj0jr+CekVdw6fWzmHj+aLcLFa3MIRdjTEVE7gPwLIBBAI8aY14VkXvrzx80xkyIyDMATgGoAfiuMeZnnSw4EfUHGbwW+x/4KgDg8IPeWc9UkJAxdBhjjgA44jx20Pn76wC+XlzRiDae3aUSzo6OdrsY1Kd4pSgRUSQY6EREkWCgExFFgoFORBQJBjoRUSQY6EREkWCgExFFgoFORBQJBjoRUSQY6EREkWCgExFFIuheLkRE7Tj13DOYeP4opl8/A2DlJl2Xyr+Dzduu7nLJ4sJAJ+oxO0szuDS8FUuDI90uSmEmnj+KS6+fxfZdNzYeWyqVAFzpXqEixEAnonVx7a7djdvoAsAj//CDLpYmThxDJyKKBAOdiCgSDHQiWnennnsGS6USqpVKt4sSFQY6Ea27ieePwlSrGBziabwiMdCJqKNMbR7Tr5/Bwq8ur3lu2Qgee+wxXLx4ERcvXsTx48e7UMJ4MNCJqLNqc1haXMTC21dw+MH7ceq5ZwAAo0NbAAzh4ptT+Pz7r+D2TT/HK6+80t2y9jkGOhF1WA0yMIDtu27EpdfPYuL5owCAr1cMPjt8Ae8d2Ia7774b1113XXeLGQEGOhF13PDoKPY/8FVcu2t3t4sSNQY6EVEkeIqZiDquVqng8IP3N+7lctW7eQ+XTmAPnYg6rlqpNO7lsrS4iCu/vNjtIkWJgU5E60Lv5XL9nt+GqdW6XZwoMdCJiCLBQCciigQDnYjWzannnmmcGKXiMdCJaN1MPH8US4uL3S5GtBjoRNRhBqZW897LhYrFQCeidXHVe65p+nvlXi6AqXDGS1EY6ETUFaMDmwEAMsQYKkpQTYrI7SLymoicFpH7U5b7PRGpisifF1dEIiIKkRnoIjII4GEAdwDYA+AuEdmTsNzXADxbdCGJiChbSA/9VgCnjTFnjDFLAJ4AsM+z3F8D+DcA0wWWj4iIAoUE+k4A562/J+uPNYjITgCfBnAw7Y1E5ICIHBcR/iwJEVHBQgJdPI8Z5+9/BvBlY0w17Y2MMYeMMXuNMXtDC0hERGFCbp87CeAG6+/rAVxwltkL4AkRAYAxAJ8SkYox5t8LKSUREWUKCfSXANwkIrsBvAngTgCfsRcwxjR+hkREvgfgPxnmRGSbfv0MapXUg3hqU2agG2MqInIfVmavDAJ41BjzqojcW38+ddyciAgAL/lfB0G/WGSMOQLgiPOYN8iNMZ9vv1hERJQXL9EiIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiwUAnIooEA52IKBIMdCKiSDDQiYgiERToInK7iLwmIqdF5H7P858VkVP1/34qIrcUX1QiIkqTGegiMgjgYQB3ANgD4C4R2eMsdhbAHxhjPgTgKwAOFV1Q6g4x3S4BEYUK6aHfCuC0MeaMMWYJwBMA9tkLGGN+aoz5Vf3PFwFcX2wxiYgoS0ig7wRw3vp7sv5Ykr8E8LTvCRE5ICLHReR4eBGJiChESKCL5zHvgbiIfAIrgf5l3/PGmEPGmL3GmL3hRSQiohBDActMArjB+vt6ABfchUTkQwC+C+AOY8xbxRSPiIhChfTQXwJwk4jsFpFNAO4E8JS9gIi8H8APAfyFMeYXxReTiIiyZPbQjTEVEbkPwLMABgE8aox5VUTurT9/EMA/AngvgG+JCABUOKxCRLS+QoZcYIw5AuCI89hB699fAPCFYotGRER58EpRIqJIMNCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDnaiPif+2SrRBMdCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDnYgoEgx0IqJIMNCJiCLBQCciigQDfSMzptslIOorYmrdLkIqBjoRUSQiDvRqtwtA1HFDbOdkCQp0EbldRF4TkdMicr/neRGRb9SfPyUiHym+qERElCYz0EVkEMDDAO4AsAfAXSKyx1nsDgA31f87AODbBZeTiIgyDAUscyuA08aYMwAgIk8A2Adg3FpmH4DvG2MMgBdF5GoR2WGMmSq8xEQUlZnlK3jsscdw8eJFLC0t4aGHHlqzzMWlDwLYhCtvH8fRH/9VrvevVh8AsKmQsg7XlrE0OFLIe3VCSKDvBHDe+nsSwG0By+wE0BToInIAKz14AJh/42t/8lqu0hJF7o1uF6AD/i7x8f/K/V5vAPjv3K/6s9yv6HEfSHoiJNDF85g73y1kGRhjDgE4FPCZRESUU8hJ0UkAN1h/Xw/gQgvLEBFRB4UE+ksAbhKR3SKyCcCdAJ5ylnkKwOfqs11+H8DbHD8nIlpfmUMuxpiKiNwH4FkAgwAeNca8KiL31p8/COAIgE8BOA1gEcDdnSsyERH5iOHl30REUYj4SlEioo2FgU5EFAkGOhFRJBjoRESRYKATEUWCgU5EFAkGOhFRJP4fvSmRtlHQKNUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "Z = linkage(matrix_sample.todense(), metric='cosine', method='complete')\n",
    "_ = dendrogram(Z, no_labels=True) # Plot dentrogram chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z_ = linkage(matrix_sample.todense()[:25], metric='cosine', method='complete')\n",
    "_ = dendrogram(Z_, no_labels=True) # Plot dentrogram chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 1 (2 docs)\n",
      "coral, decadal, ramsay, onoctober, algebraists, researcherswith, tenth, theholocene, seminar, solomon\n",
      "\n",
      "Cluster: 2 (6 docs)\n",
      "wavelet, jet, harmonic, jets, transverse, isotropic, subgrid, piv, frame, controlmethodologies\n",
      "\n",
      "Cluster: 3 (7 docs)\n",
      "bubble, gravitational, plume, envelope, radiation, ligo, inspiral, wave, binaries, star\n",
      "\n",
      "Cluster: 4 (4 docs)\n",
      "straightening, heat, repair, cores, damaged, effectofheat, definethelimiting, basedonexperience, applicationisprimarily, applicationforother\n",
      "\n",
      "Cluster: 5 (13 docs)\n",
      "helium, droplets, superfluid, liquid, 3he, multipole, molecules, atomic, areproposed, turbulence\n",
      "\n",
      "Cluster: 6 (16 docs)\n",
      "workshop, forensic, kdi, pbo, gaim, workshops, awards, lawyers, quebec, npp\n",
      "\n",
      "Cluster: 7 (9 docs)\n",
      "speckle, brain, tumors, ligand, mri, imaging, fmri, neuropeptides, synaptic, pk\n",
      "\n",
      "Cluster: 8 (35 docs)\n",
      "uml, polymer, biology, museum, figurines, yeara, umebproject, progressmonitored, ofbroadening, breaksand\n",
      "\n",
      "Cluster: 9 (15 docs)\n",
      "accessibility, cognition, database, macroeconomics, meetings, multimedia, elderly, ssp, pri, expeditions\n",
      "\n",
      "Cluster: 10 (41 docs)\n",
      "control, sparse, customer, linear, matrices, printing, dense, locomotion, nonlinear, algebra\n",
      "\n",
      "Cluster: 11 (51 docs)\n",
      "composite, parallel, amorphous, grid, ultrabroadband, processing, optical, cad, fiber, diamond\n",
      "\n",
      "Cluster: 12 (18 docs)\n",
      "clay, mineralogy, subduction, shear, dye, friction, deformation, shales, grain, alloys\n",
      "\n",
      "Cluster: 13 (20 docs)\n",
      "films, micro, friction, lubrication, metrology, slip, deposition, lsp, cell, fluid\n",
      "\n",
      "Cluster: 14 (46 docs)\n",
      "magnetic, quantum, superconductors, spin, superconducting, photon, photodissociation, dots, polarized, condensed\n",
      "\n",
      "Cluster: 15 (5 docs)\n",
      "particles, particle, thisapproach, macromolecules, shinbrot, problemthrough, cellularautomata, sheared, separation, flowing\n",
      "\n",
      "Cluster: 16 (7 docs)\n",
      "sediment, floodplain, clinoform, strickland, immiscible, fly, trapping, thestrickland, diffusivity, floodplains\n",
      "\n",
      "Cluster: 17 (6 docs)\n",
      "fuel, xenon, chamber, module, pemfc, ice, fc, logger, amanda, aliquid\n",
      "\n",
      "Cluster: 18 (18 docs)\n",
      "bone, muscle, seismic, cardiac, earthquake, walls, ultrasound, stimulation, damage, stretch\n",
      "\n",
      "Cluster: 19 (12 docs)\n",
      "cmes, solar, wind, cme, coronal, lake, coronagraph, mesoscale, shock, tunnels\n",
      "\n",
      "Cluster: 20 (11 docs)\n",
      "terrorist, attacks, attitudes, wtc, trust, public, polling, september, immigration, polls\n",
      "\n",
      "Cluster: 21 (11 docs)\n",
      "adolescents, tourism, chinese, networks, texts, friendship, newspaper, tourist, beirut, organizational\n",
      "\n",
      "Cluster: 22 (85 docs)\n",
      "algebras, manifolds, conformal, algebraic, lie, curvature, spaces, ergodic, quantum, symmetries\n",
      "\n",
      "Cluster: 23 (16 docs)\n",
      "stark, conjectures, automorphic, forms, arithmetic, howe, infection, virus, screening, perfect\n",
      "\n",
      "Cluster: 24 (42 docs)\n",
      "equations, schrodinger, dynamical, regularity, ampere, singularities, monge, waves, cauchy, pde\n",
      "\n",
      "Cluster: 25 (15 docs)\n",
      "regression, minkowski, sets, estimators, tangent, theory, censored, curves, analogy, norm\n",
      "\n",
      "Cluster: 26 (9 docs)\n",
      "quantum, classification, he, redundant, perceptrons, image, approximation, tcs, dm, theclassification\n",
      "\n",
      "Cluster: 27 (13 docs)\n",
      "script, cyclogenesis, kona, lows, nova, tiwanaku, tropical, valley, peoples, car\n",
      "\n",
      "Cluster: 28 (1 docs)\n",
      "anthracis, bacillus, outbreak, thenbe, quest, anthrax, sequence, strains, genomic, florida\n",
      "\n",
      "Cluster: 29 (8 docs)\n",
      "sensor, rehabilitation, vr, ankle, emergencies, diagnostic, rc, assets, retrofit, integrity\n",
      "\n",
      "Cluster: 30 (5 docs)\n",
      "chile, universidad, visit, carlos, brazil, inpe, rankin, estadual, dunn, thisplanning\n",
      "\n",
      "Cluster: 31 (5 docs)\n",
      "voting, circuits, asu, telecommunication, universityrequests, advancement, grad, ballot, uthe, resultshave\n",
      "\n",
      "Cluster: 32 (9 docs)\n",
      "asymmetric, catalytic, ions, heteroatom, bonds, carbon, membered, aromaticity, phthalocyanines, phosphorus\n",
      "\n",
      "Cluster: 33 (13 docs)\n",
      "instrument, isotope, canopy, microscope, stable, ecosystems, japan, ecosystemecology, fatty, macromolecules\n",
      "\n",
      "Cluster: 34 (13 docs)\n",
      "conference, speakers, mg2, nanotechnology, clinicians, lasers, theconference, japan, panel, lecture\n",
      "\n",
      "Cluster: 35 (13 docs)\n",
      "oceanographic, vessel, ship, ships, equipment, operated, nationaloceanographic, researchfleet, vessels, profiler\n",
      "\n",
      "Cluster: 36 (6 docs)\n",
      "collider, neutrinos, detector, babar, quark, fermilab, rsvp, cp, accident, neutrino\n",
      "\n",
      "Cluster: 37 (17 docs)\n",
      "climate, madagascar, aerosol, ices, lemurs, indoex, pacific, bison, globec, global\n",
      "\n",
      "Cluster: 38 (4 docs)\n",
      "laminar, table, centrifuge, box, nied, 1g, shaking, rpi, pile, ucsd\n",
      "\n",
      "Cluster: 39 (5 docs)\n",
      "resistivity, 3d, sinkholes, cave, cheat, magma, cheating, 2d, ridge, void\n",
      "\n",
      "Cluster: 40 (18 docs)\n",
      "crcd, curriculumdevelopment, dame, notre, underthe, pittsburgh, entitled, communications, mexico, ut\n",
      "\n",
      "Cluster: 41 (5 docs)\n",
      "czech, branes, string, homotopy, phenomenology, cytokinins, cosmology, spain, vanadium, gt\n",
      "\n",
      "Cluster: 42 (12 docs)\n",
      "assistive, disabilities, designprojects, bio, curriculum, needs, wyoming, xp, smart, tcu\n",
      "\n",
      "Cluster: 43 (3 docs)\n",
      "abstract, required, no, soap, morgan, baldwin, universe, hrushovski, thetechnique, universes\n",
      "\n",
      "Cluster: 44 (39 docs)\n",
      "algebraic, algebra, varieties, commutative, curves, geometry, spaces, hilbert, ideals, galois\n",
      "\n",
      "Cluster: 45 (94 docs)\n",
      "fellowship, mathematical, sciences, postdoctoral, training, twenty, dr, biology, microbial, he\n",
      "\n",
      "Cluster: 46 (44 docs)\n",
      "available, not, unit, prosthesis, asensor, nrl, implantable, implantation, darpa, retinal\n",
      "\n",
      "Cluster: 47 (7 docs)\n",
      "arc, gulf, mantle, thesubduction, wedge, rifts, basement, beingcarried, tectonically, khz\n",
      "\n",
      "Cluster: 48 (13 docs)\n",
      "magnetosphere, magnetic, particle, ionosphere, auroral, relativistic, ring, ions, ringcurrent, mhd\n",
      "\n",
      "Cluster: 49 (6 docs)\n",
      "china, qing, modernity, political, men, government, governmentthis, democratic, kennedy, executives\n",
      "\n",
      "Cluster: 50 (8 docs)\n",
      "indigenous, town, women, magazine, lactation, janice, aspray, pilot, divisions, cs\n",
      "\n",
      "Cluster: 51 (17 docs)\n",
      "sdp, resultants, nlp, matrix, polynomial, ip, polynomials, linear, var, problemswhile\n",
      "\n",
      "Cluster: 52 (2 docs)\n",
      "plasma, pulse, laser, sra, semiconductor, fs, amplification, tuning, discharge, clean\n",
      "\n",
      "Cluster: 53 (9 docs)\n",
      "iron, arsenic, corrosion, phosphorus, mobilization, sediments, ofreduced, timescales, perfluorinated, reduced\n",
      "\n",
      "Cluster: 54 (11 docs)\n",
      "symposium, children, library, 28th, wise, melbourne, representation, seminar, meeting, vancouver\n",
      "\n",
      "Cluster: 55 (68 docs)\n",
      "teachers, college, colleges, mathematics, teacher, alliance, teaching, manufacturing, curriculum, technical\n",
      "\n",
      "Cluster: 56 (11 docs)\n",
      "fellowships, mathematical, sciences, nigms, rna, genes, ribozymes, antisense, rnas, nigmsinitiative\n",
      "\n",
      "Cluster: 57 (1 docs)\n",
      "contract, zygotic, zygomycota, zygomycetes, zygmundoperators, zygmund, zworski, zurich, zuni, zro2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters = fcluster(Z, 0.99, criterion='distance') # Create flat clusters by distance threshold\n",
    "\n",
    "print_clusters(matrix_sample, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 13:19:20,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-02-23 13:19:22,649 : INFO : built Dictionary(185699 unique tokens: ['Abelian', 'Anancient', 'Chow', 'Chowgroup', 'Conjecture']...) from 9923 documents (total 2564109 corpus positions)\n",
      "2021-02-23 13:19:24,031 : INFO : using symmetric alpha at 0.1\n",
      "2021-02-23 13:19:24,031 : INFO : using symmetric eta at 0.1\n",
      "2021-02-23 13:19:24,059 : INFO : using serial LDA version on this node\n",
      "2021-02-23 13:19:24,226 : INFO : running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 9923 documents, updating model once every 2000 documents, evaluating perplexity every 9923 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2021-02-23 13:19:24,227 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2021-02-23 13:19:24,227 : INFO : PROGRESS: pass 0, at document #2000/9923\n",
      "2021-02-23 13:19:26,043 : INFO : merging changes from 2000 documents into a model of 9923 documents\n",
      "2021-02-23 13:19:26,178 : INFO : topic #0 (0.100): 0.040*\"of\" + 0.038*\"and\" + 0.037*\"the\" + 0.021*\"in\" + 0.020*\"to\" + 0.009*\"for\" + 0.009*\"The\" + 0.008*\"on\" + 0.007*\"will\" + 0.007*\"are\"\n",
      "2021-02-23 13:19:26,181 : INFO : topic #5 (0.100): 0.042*\"and\" + 0.041*\"the\" + 0.035*\"of\" + 0.031*\"to\" + 0.014*\"in\" + 0.010*\"for\" + 0.010*\"will\" + 0.009*\"as\" + 0.008*\"is\" + 0.008*\"The\"\n",
      "2021-02-23 13:19:26,183 : INFO : topic #4 (0.100): 0.043*\"the\" + 0.030*\"of\" + 0.028*\"and\" + 0.018*\"to\" + 0.011*\"will\" + 0.010*\"in\" + 0.010*\"is\" + 0.009*\"The\" + 0.008*\"be\" + 0.007*\"on\"\n",
      "2021-02-23 13:19:26,185 : INFO : topic #1 (0.100): 0.054*\"the\" + 0.046*\"of\" + 0.039*\"and\" + 0.023*\"in\" + 0.020*\"to\" + 0.014*\"is\" + 0.011*\"The\" + 0.011*\"for\" + 0.010*\"will\" + 0.007*\"be\"\n",
      "2021-02-23 13:19:26,188 : INFO : topic #7 (0.100): 0.043*\"the\" + 0.043*\"of\" + 0.033*\"and\" + 0.027*\"to\" + 0.017*\"in\" + 0.013*\"is\" + 0.010*\"The\" + 0.010*\"will\" + 0.009*\"be\" + 0.009*\"for\"\n",
      "2021-02-23 13:19:26,193 : INFO : topic diff=7.753490, rho=1.000000\n",
      "2021-02-23 13:19:26,196 : INFO : PROGRESS: pass 0, at document #4000/9923\n",
      "2021-02-23 13:19:28,044 : INFO : merging changes from 2000 documents into a model of 9923 documents\n",
      "2021-02-23 13:19:28,134 : INFO : topic #4 (0.100): 0.029*\"the\" + 0.020*\"of\" + 0.019*\"and\" + 0.012*\"to\" + 0.007*\"will\" + 0.007*\"in\" + 0.007*\"is\" + 0.006*\"The\" + 0.005*\"be\" + 0.005*\"on\"\n",
      "2021-02-23 13:19:28,136 : INFO : topic #3 (0.100): 0.047*\"of\" + 0.046*\"the\" + 0.028*\"and\" + 0.024*\"to\" + 0.022*\"in\" + 0.011*\"will\" + 0.009*\"that\" + 0.009*\"The\" + 0.008*\"for\" + 0.008*\"be\"\n",
      "2021-02-23 13:19:28,138 : INFO : topic #7 (0.100): 0.046*\"the\" + 0.045*\"of\" + 0.033*\"and\" + 0.028*\"to\" + 0.018*\"in\" + 0.012*\"will\" + 0.012*\"is\" + 0.011*\"be\" + 0.010*\"The\" + 0.008*\"that\"\n",
      "2021-02-23 13:19:28,140 : INFO : topic #1 (0.100): 0.060*\"the\" + 0.050*\"of\" + 0.042*\"and\" + 0.025*\"in\" + 0.023*\"to\" + 0.014*\"is\" + 0.013*\"will\" + 0.012*\"The\" + 0.011*\"for\" + 0.009*\"be\"\n",
      "2021-02-23 13:19:28,143 : INFO : topic #5 (0.100): 0.045*\"and\" + 0.043*\"the\" + 0.037*\"of\" + 0.033*\"to\" + 0.016*\"in\" + 0.012*\"will\" + 0.012*\"for\" + 0.010*\"as\" + 0.009*\"The\" + 0.008*\"is\"\n",
      "2021-02-23 13:19:28,148 : INFO : topic diff=0.832221, rho=0.707107\n",
      "2021-02-23 13:19:28,151 : INFO : PROGRESS: pass 0, at document #6000/9923\n",
      "2021-02-23 13:19:30,019 : INFO : merging changes from 2000 documents into a model of 9923 documents\n",
      "2021-02-23 13:19:30,113 : INFO : topic #4 (0.100): 0.018*\"the\" + 0.013*\"of\" + 0.012*\"and\" + 0.008*\"to\" + 0.005*\"will\" + 0.004*\"in\" + 0.004*\"is\" + 0.004*\"The\" + 0.003*\"be\" + 0.003*\"on\"\n",
      "2021-02-23 13:19:30,115 : INFO : topic #7 (0.100): 0.044*\"the\" + 0.044*\"of\" + 0.033*\"and\" + 0.027*\"to\" + 0.018*\"in\" + 0.013*\"will\" + 0.011*\"is\" + 0.010*\"be\" + 0.009*\"The\" + 0.009*\"that\"\n",
      "2021-02-23 13:19:30,117 : INFO : topic #2 (0.100): 0.037*\"of\" + 0.028*\"the\" + 0.022*\"and\" + 0.021*\"to\" + 0.014*\"in\" + 0.013*\"is\" + 0.010*\"The\" + 0.009*\"for\" + 0.009*\"that\" + 0.008*\"on\"\n",
      "2021-02-23 13:19:30,119 : INFO : topic #5 (0.100): 0.049*\"and\" + 0.042*\"the\" + 0.039*\"of\" + 0.035*\"to\" + 0.017*\"in\" + 0.013*\"will\" + 0.013*\"for\" + 0.010*\"research\" + 0.010*\"The\" + 0.009*\"as\"\n",
      "2021-02-23 13:19:30,122 : INFO : topic #6 (0.100): 0.059*\"of\" + 0.035*\"the\" + 0.031*\"and\" + 0.023*\"in\" + 0.022*\"to\" + 0.020*\"will\" + 0.012*\"be\" + 0.008*\"for\" + 0.008*\"is\" + 0.007*\"The\"\n",
      "2021-02-23 13:19:30,128 : INFO : topic diff=0.673728, rho=0.577350\n",
      "2021-02-23 13:19:30,130 : INFO : PROGRESS: pass 0, at document #8000/9923\n",
      "2021-02-23 13:19:31,908 : INFO : merging changes from 2000 documents into a model of 9923 documents\n",
      "2021-02-23 13:19:31,999 : INFO : topic #6 (0.100): 0.057*\"of\" + 0.033*\"the\" + 0.030*\"and\" + 0.022*\"in\" + 0.021*\"to\" + 0.020*\"will\" + 0.011*\"be\" + 0.008*\"for\" + 0.007*\"is\" + 0.007*\"The\"\n",
      "2021-02-23 13:19:32,002 : INFO : topic #2 (0.100): 0.035*\"of\" + 0.025*\"the\" + 0.020*\"to\" + 0.019*\"and\" + 0.013*\"in\" + 0.012*\"is\" + 0.009*\"The\" + 0.008*\"for\" + 0.008*\"that\" + 0.007*\"on\"\n",
      "2021-02-23 13:19:32,004 : INFO : topic #7 (0.100): 0.043*\"the\" + 0.043*\"of\" + 0.032*\"and\" + 0.026*\"to\" + 0.018*\"in\" + 0.012*\"will\" + 0.011*\"is\" + 0.009*\"that\" + 0.009*\"be\" + 0.008*\"The\"\n",
      "2021-02-23 13:19:32,007 : INFO : topic #5 (0.100): 0.051*\"and\" + 0.043*\"the\" + 0.040*\"of\" + 0.035*\"to\" + 0.017*\"in\" + 0.014*\"for\" + 0.013*\"will\" + 0.010*\"The\" + 0.009*\"is\" + 0.009*\"as\"\n",
      "2021-02-23 13:19:32,009 : INFO : topic #0 (0.100): 0.022*\"of\" + 0.021*\"and\" + 0.020*\"the\" + 0.012*\"in\" + 0.011*\"to\" + 0.005*\"for\" + 0.005*\"The\" + 0.005*\"on\" + 0.004*\"will\" + 0.004*\"are\"\n",
      "2021-02-23 13:19:32,016 : INFO : topic diff=0.484399, rho=0.500000\n",
      "2021-02-23 13:19:34,851 : INFO : -9.108 per-word bound, 551.9 perplexity estimate based on a held-out corpus of 1923 documents with 502309 words\n",
      "2021-02-23 13:19:34,851 : INFO : PROGRESS: pass 0, at document #9923/9923\n",
      "2021-02-23 13:19:36,508 : INFO : merging changes from 1923 documents into a model of 9923 documents\n",
      "2021-02-23 13:19:36,598 : INFO : topic #9 (0.100): 0.037*\"the\" + 0.033*\"of\" + 0.018*\"and\" + 0.015*\"in\" + 0.013*\"to\" + 0.007*\"will\" + 0.005*\"is\" + 0.005*\"for\" + 0.005*\"The\" + 0.005*\"that\"\n",
      "2021-02-23 13:19:36,601 : INFO : topic #6 (0.100): 0.058*\"of\" + 0.034*\"the\" + 0.030*\"and\" + 0.021*\"in\" + 0.020*\"will\" + 0.020*\"to\" + 0.011*\"be\" + 0.007*\"for\" + 0.007*\"is\" + 0.007*\"The\"\n",
      "2021-02-23 13:19:36,603 : INFO : topic #4 (0.100): 0.007*\"the\" + 0.005*\"of\" + 0.004*\"and\" + 0.003*\"to\" + 0.002*\"will\" + 0.002*\"in\" + 0.002*\"is\" + 0.001*\"The\" + 0.001*\"be\" + 0.001*\"on\"\n",
      "2021-02-23 13:19:36,605 : INFO : topic #3 (0.100): 0.030*\"of\" + 0.030*\"the\" + 0.019*\"and\" + 0.016*\"to\" + 0.014*\"in\" + 0.007*\"will\" + 0.006*\"that\" + 0.006*\"The\" + 0.005*\"for\" + 0.005*\"be\"\n",
      "2021-02-23 13:19:36,607 : INFO : topic #2 (0.100): 0.039*\"of\" + 0.027*\"the\" + 0.019*\"to\" + 0.018*\"and\" + 0.014*\"is\" + 0.014*\"in\" + 0.012*\"theory\" + 0.010*\"The\" + 0.008*\"problems\" + 0.008*\"on\"\n",
      "2021-02-23 13:19:36,612 : INFO : topic diff=0.347209, rho=0.447214\n"
     ]
    }
   ],
   "source": [
    "## Topic modeling demo\n",
    "#!pip3 install gensim\n",
    "\n",
    "# Fast and simple tokenization\n",
    "new_vectorizer = TfidfVectorizer()\n",
    "word_tokenizer = new_vectorizer.build_tokenizer()\n",
    "tokenized_text = [word_tokenizer(doc) for doc in abstracts]\n",
    "\n",
    "# Train LDA model\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "lda_model = models.LdaModel(lda_corpus, id2word=dictionary, num_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar terms to: accompany\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 'accompany'),\n",
       " (0.3489035084703193, 'brochure'),\n",
       " (0.271665382200952, 'useindicators'),\n",
       " (0.271665382200952, 'socialdemographics'),\n",
       " (0.271665382200952, 'shiftsthe'),\n",
       " (0.271665382200952, 'scientificmeasurement'),\n",
       " (0.271665382200952, 'ofsustainable'),\n",
       " (0.271665382200952, 'internationalorganizations'),\n",
       " (0.271665382200952, 'humaninstitutions'),\n",
       " (0.271665382200952, 'grapple')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "term_index = 1000\n",
    "print(\"Similar terms to:\", features[term_index])\n",
    "# Get most similar terms according to the cosine similarity of their vectors (columns in the term-document matrix)\n",
    "heapq.nlargest(10, zip(cosine_similarity(tfidf_matrix[:,term_index].todense().T, tfidf_matrix.todense().T)[0], features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 14:53:14,180 : INFO : collecting all words and their counts\n",
      "2021-02-23 14:53:14,181 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-23 14:53:14,678 : INFO : collected 185699 word types from a corpus of 2564109 raw words and 9923 sentences\n",
      "2021-02-23 14:53:14,679 : INFO : Loading a fresh vocabulary\n",
      "2021-02-23 14:53:14,921 : INFO : effective_min_count=3 retains 42939 unique words (23% of original 185699, drops 142760)\n",
      "2021-02-23 14:53:14,922 : INFO : effective_min_count=3 leaves 2392825 word corpus (93% of original 2564109, drops 171284)\n",
      "2021-02-23 14:53:15,057 : INFO : deleting the raw counts dictionary of 185699 items\n",
      "2021-02-23 14:53:15,059 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2021-02-23 14:53:15,060 : INFO : downsampling leaves estimated 1880085 word corpus (78.6% of prior 2392825)\n",
      "2021-02-23 14:53:15,166 : INFO : estimated required memory for 42939 words and 100 dimensions: 55820700 bytes\n",
      "2021-02-23 14:53:15,167 : INFO : resetting layer weights\n",
      "2021-02-23 14:53:24,047 : INFO : training model with 4 workers on 42939 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-02-23 14:53:25,056 : INFO : EPOCH 1 - PROGRESS: at 84.23% examples, 1568592 words/s, in_qsize 8, out_qsize 0\n",
      "2021-02-23 14:53:25,226 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-23 14:53:25,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-23 14:53:25,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-23 14:53:25,238 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-23 14:53:25,239 : INFO : EPOCH - 1 : training on 2564109 raw words (1879123 effective words) took 1.2s, 1580876 effective words/s\n",
      "2021-02-23 14:53:26,243 : INFO : EPOCH 2 - PROGRESS: at 83.19% examples, 1563155 words/s, in_qsize 7, out_qsize 0\n",
      "2021-02-23 14:53:26,442 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-23 14:53:26,455 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-23 14:53:26,456 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-23 14:53:26,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-23 14:53:26,457 : INFO : EPOCH - 2 : training on 2564109 raw words (1880329 effective words) took 1.2s, 1548192 effective words/s\n",
      "2021-02-23 14:53:27,463 : INFO : EPOCH 3 - PROGRESS: at 82.79% examples, 1554117 words/s, in_qsize 8, out_qsize 0\n",
      "2021-02-23 14:53:27,652 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-23 14:53:27,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-23 14:53:27,664 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-23 14:53:27,666 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-23 14:53:27,667 : INFO : EPOCH - 3 : training on 2564109 raw words (1879934 effective words) took 1.2s, 1559622 effective words/s\n",
      "2021-02-23 14:53:28,672 : INFO : EPOCH 4 - PROGRESS: at 87.95% examples, 1648332 words/s, in_qsize 7, out_qsize 0\n",
      "2021-02-23 14:53:28,798 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-23 14:53:28,801 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-23 14:53:28,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-23 14:53:28,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-23 14:53:28,812 : INFO : EPOCH - 4 : training on 2564109 raw words (1880790 effective words) took 1.1s, 1646772 effective words/s\n",
      "2021-02-23 14:53:29,823 : INFO : EPOCH 5 - PROGRESS: at 86.52% examples, 1608346 words/s, in_qsize 7, out_qsize 0\n",
      "2021-02-23 14:53:29,963 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-23 14:53:29,966 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-23 14:53:29,968 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-23 14:53:29,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-23 14:53:29,976 : INFO : EPOCH - 5 : training on 2564109 raw words (1880405 effective words) took 1.2s, 1619273 effective words/s\n",
      "2021-02-23 14:53:29,977 : INFO : training on a 12820545 raw words (9400581 effective words) took 5.9s, 1585485 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim # Make sure you also have cython installed to accelerate computation!\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train word2vec model\n",
    "vectors = gensim.models.Word2Vec(tokenized_text, size=100, window=5, min_count=3, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.6765823 , -0.7136156 , -0.2387944 ,  0.253341  , -1.529553  ,\n",
       "       -0.48159742,  0.24520285, -0.6345149 , -0.74827284,  0.5025335 ,\n",
       "        0.34621516, -0.8036435 ,  0.13782643, -0.19438858,  0.9572365 ,\n",
       "       -0.05627961, -0.02325015, -1.4054319 ,  1.3312975 , -1.1004852 ,\n",
       "       -0.20337525, -0.7955262 ,  0.07213792,  0.26370227,  0.19093844,\n",
       "       -0.9250585 , -0.9552183 ,  0.14507355,  0.0904886 ,  1.7888969 ,\n",
       "       -0.6325377 , -0.3666335 ,  2.4750755 , -2.0613616 , -0.8378274 ,\n",
       "        1.2740463 ,  0.966118  ,  0.26258188, -0.02977295,  1.3109517 ,\n",
       "       -0.42681482,  1.2936581 , -0.01875491, -0.0310684 , -0.47264764,\n",
       "        0.40155366, -0.19718994, -0.66719776, -0.08188625, -1.0294431 ,\n",
       "        0.32254252, -0.11602645,  0.15241462,  0.16647631, -1.3818822 ,\n",
       "        0.7330966 , -0.43827465, -0.01911481, -0.48765445, -0.9952913 ,\n",
       "       -0.577614  ,  0.36340496,  1.3637257 , -0.22923048, -0.08211825,\n",
       "       -0.8066844 ,  0.22615565, -0.4948494 ,  0.6948285 ,  1.2707686 ,\n",
       "        0.48930407, -1.1316185 , -0.5277863 ,  0.9239728 , -0.309272  ,\n",
       "        0.12436585,  0.73524994, -0.05509188,  0.9737807 , -0.01615185,\n",
       "       -0.7182479 ,  0.6239305 ,  0.35097733, -0.05497223, -0.62738585,\n",
       "        0.07318493, -0.5523001 ,  0.5135454 ,  0.5431706 ,  0.5057951 ,\n",
       "       -0.95260215,  1.0957065 ,  0.860503  ,  0.7274576 ,  1.2366637 ,\n",
       "       -0.21114476, -0.22023514,  0.5336453 , -0.39458555, -0.37049362],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a word vector\n",
    "vectors.wv['process']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 13:20:14,894 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: process\n",
      "[('strategy', 0.7501661777496338), ('mechanism', 0.7261341214179993), ('architecture', 0.7232430577278137), ('product', 0.6989142298698425), ('system', 0.6746004819869995), ('feedback', 0.648912787437439), ('itself', 0.6441490650177002), ('control', 0.6368333101272583), ('tool', 0.6344780325889587), ('paradigm', 0.6231105327606201)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect words with vectors most similar to a given word\n",
    "print(\"Most similar to:\", 'process')\n",
    "print(vectors.wv.most_similar('process'))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to: ['process', 'strategy']\n",
      "[('architecture', 0.7967891097068787), ('product', 0.7658562064170837), ('paradigm', 0.7474970817565918), ('tool', 0.7378526926040649), ('framework', 0.737643301486969), ('methodology', 0.7372544407844543), ('protocol', 0.7324867248535156), ('approach', 0.7261338233947754), ('system', 0.7186410427093506), ('feedback', 0.7108350992202759)]\n"
     ]
    }
   ],
   "source": [
    "# ...or combination or words (average vector)\n",
    "print(\"Most similar to:\", ['process', 'strategy'])\n",
    "print(vectors.wv.most_similar(['process', 'strategy']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681544\n",
      "0.3842877\n",
      "0.4579664\n",
      "0.6073289\n"
     ]
    }
   ],
   "source": [
    "# Inspect cosine similarities between specific words\n",
    "print(vectors.wv.similarity('technology', 'infrastructure'))\n",
    "print(vectors.wv.similarity('technology', 'system'))\n",
    "print(vectors.wv.similarity('technology', 'consumer'))\n",
    "print(vectors.wv.similarity('technology', 'advanced'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mechanisms', 'microbes', 'academe', 'buffers', 'simultaneous']\n"
     ]
    }
   ],
   "source": [
    "words = [list(vectors.wv.vocab.keys())[(i+1)*1100] for i in range(5)]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-23 14:53:30,015 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('processes', 0.8601334095001221), ('pathways', 0.8588832020759583), ('factors', 0.8325076103210449), ('mechanism', 0.8167534470558167), ('regulation', 0.7949987649917603), ('themechanisms', 0.7842873334884644), ('behavior', 0.7744489908218384), ('evolution', 0.7599372267723083), ('formation', 0.7562920451164246), ('forces', 0.7544025182723999)]\n",
      "[('algae', 0.9182683229446411), ('microorganisms', 0.9128854274749756), ('fish', 0.9121999740600586), ('cultivated', 0.9026154279708862), ('assemblages', 0.9025028347969055), ('fungi', 0.8995836973190308), ('planets', 0.8942093849182129), ('constituents', 0.891696572303772), ('A1', 0.88628089427948), ('bacteria', 0.8835186958312988)]\n",
      "[('officials', 0.8861537575721741), ('academia', 0.884754478931427), ('forcareers', 0.8827353715896606), ('problemsolving', 0.8810858726501465), ('entrepreneurship', 0.8780913949012756), ('forwomen', 0.8730876445770264), ('governmentlaboratories', 0.8609949350357056), ('entrepreneurs', 0.8586121201515198), ('interpreters', 0.856927752494812), ('toassist', 0.8537488579750061)]\n",
      "[('columns', 0.9016135931015015), ('reproducibly', 0.8971738219261169), ('casting', 0.8933674097061157), ('electrodes', 0.8892380595207214), ('ultraviolet', 0.8872681856155396), ('multilayers', 0.8871352076530457), ('rays', 0.885539710521698), ('coated', 0.8827422261238098), ('annealing', 0.8814632296562195), ('lasing', 0.8807177543640137)]\n",
      "[('tuning', 0.8956936597824097), ('binary', 0.8937495350837708), ('2D', 0.8882228136062622), ('auxiliaryinformation', 0.885289192199707), ('reflection', 0.8848088383674622), ('nanoscopic', 0.8816380500793457), ('walkthroughs', 0.8815721273422241), ('layered', 0.8809983730316162), ('anoptical', 0.8806330561637878), ('alignment', 0.8795812129974365)]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.wv.most_similar(words[0]))\n",
    "print(vectors.wv.most_similar(words[1]))\n",
    "print(vectors.wv.most_similar(words[2]))\n",
    "print(vectors.wv.most_similar(words[3]))\n",
    "print(vectors.wv.most_similar(words[4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
